type: install
version: 1.5
id: kubernetes-release
baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
description:
  text: /text/description-kube.md
  short: Kubernetes cluster with automated scaling & cost efficient pay-per-use pricing for running cloud-native microservices.
categories:
  - apps/clusters
  - apps/dev-and-admin-tools

logo: /images/k8s-logo.png
name: Kubernetes Cluster v1.18.18
targetRegions:
  type: vz7

ssl: true
onBeforeInit: /scripts/beforeinit.js
onBeforeInstall: /scripts/beforeinstall.js

nodes: definedInOnBeforeInstall

skipNodeEmails: true

globals:
    dashboardUrl:  https://${env.domain}/kubernetes-dashboard/

onInstall:
  - set-scaling-parameters
  - prepare-instances
  - init-main-master
  - init-slave-masters
  - connect-workers: cp
  - apply-worker-config: ${nodes.cp.join(id,)}
  - setup-overlay
  - if ('${settings.ingress-controller}' == 'Nginx'):
      - setGlobals:
          ingress-dir: nginx
      - install-nginx
  - elif ('${settings.ingress-controller}' == 'HAProxy'):
      - setGlobals:
          ingress-dir: haproxy
      - install-haproxy
  - else:
      - setGlobals:
          ingress-dir: traefik
      - install-traefik
  - install-components
  - install-helm-master
  - if ('${globals.k8sm-slave-ids}'):
      install-helm-slave: ${globals.k8sm-slave-ids}
  - generate-admin-token
  - helm-components
  - deploy

  - if (${settings.api:true}):
      - setup-remote-api: true

  - if (${settings.monitoring:false}):
        - install-monitoring

  - if (${settings.jaeger:false}):
        - install-jaeger

  - if ('${env.protocol}' == 'http'):
      - api: env.control.AddEndpoint
        nodeId: ${nodes.cp.master.id}
        privatePort: 30777
        protocol: TCP
        name: Dashboard Self-Signed HTTPS
      - setGlobals:
          dashboardUrl: https://node${nodes.cp.master.id}-${env.domain}:${response.object.publicPort}/

  - setGlobals:
        default_success: |
             **Cluster URL:** <${env.protocol}://${env.domain}>

             Enter [Kubernetes dashboard](${globals.dashboardUrl}) ${globals.default_api:} using the Access Token:
             <pre style="height: 64px;overflow: auto;"><code style="white-space: normal;">${globals.token}</code></pre>
  - check-health

onAfterScaleOut[cp]:
  - cmd[${nodes.k8sm.master.id}]: |-
      token_age=$(expr $(date +%s) - $(stat /var/log/kubeadm-init.log -c %Y))
      [ ${token_age} -lt $((20*60*60)) ] && { sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep -v 'control-plane'; } || { kubeadm token create --print-join-command; }
  - setGlobals:
      worker_join_cmd: ${response.out}
  - set:
      nodes: ${event.response.nodes.join(id,)}
      ips: ${event.response.nodes.join(extIPs,)}
  - prepare-worker-integration
  - cmd [${this.nodes}]: init-instance.sh --type=worker --initial=true --base-url=$(echo '${baseUrl}' | base64 -w 0)
  - connect-workers: ${this.nodes}
  - apply-worker-config: ${this.nodes}
  - if ('${this.ips}'):
      cmd[${nodes.k8sm.master.id}]: metallb-config -a '${this.ips}'

onBeforeScaleIn[cp]:
  forEach(event.response.nodes):
    remove-worker:
      workerHostname: node${@i.id}-${env.domain}
      workerExtIPs: ${@i.extIPs.join(,)}

onBeforeClone:
  stopEvent:
    type: warning
    message: Kubernetes Cluster cloning is not supported yet!

onBeforeMigrate:
  stopEvent:
    type: warning
    message: Kubernetes Cluster migration is not supported!

onBeforeAttachExtIp[k8sm]: block-ip-assignment

onBeforeAttachExtIp[mbl]: block-ip-assignment

onBeforeSetExtIpCount[k8sm]: block-ip-assignment

onBeforeSetExtIpCount[mbl]: block-ip-assignment

onAfterSetExtIpCount[cp]:
  - log: 'attached ips: ${event.response.attachedIps.join(,)}, detached ips: ${event.response.detachedIps.join(,)}'
  - cmd[${nodes.k8sm.master.id}]: metallb-config -a '${event.response.attachedIps.join(,)}' -d '${event.response.detachedIps.join(,)}'
  - cmd[cp]: systemctl restart kube-config.service

onBeforeDetachExtIp[cp]:
  - log: 'detached ip: ${event.params.ip}'
  - cmd[${nodes.k8sm.master.id}]: metallb-config -d '${event.params.ip}'
  - cmd[cp]: systemctl restart kube-config.service

onAfterAttachExtIp[cp]:
  - if (${event.response.result:1} == 0):
      - log: 'attached ip: ${event.response.object}'
      - cmd[${nodes.k8sm.master.id}]: metallb-config -a '${event.response.object}'
      - cmd[cp]: systemctl restart kube-config.service

actions:
  set-scaling-parameters:
    - env.control.ApplyNodeGroupData[k8sm]:
        data:
          validation:
            minCount: ${nodes.k8sm.length}
            maxCount: ${nodes.k8sm.length}
    - if (${nodes.storage.length:0} > 0):
        - script: |
            return { result: 0, min: ${nodes.storage.length:0} > 1 ? 3: 1, max: ${nodes.storage.length:0} > 1 ? 7: 1 }
        - env.control.ApplyNodeGroupData[storage]:
            data:
              validation:
                countIncrement: 2
                minCount: ${response.min}
                maxCount: ${response.max}

  block-ip-assignment:
    stopEvent:
      type: warning
      message: Kubernetes service instances shouldn't have external IPs assigned.

  prepare-instances:
    execCmd:
      - nodeGroup: k8sm
        commands: init-instance.sh --type=master --initial=true --base-url=$(echo '${baseUrl}' | base64 -w 0)
      - nodeGroup: cp
        commands: init-instance.sh --type=worker --initial=true --base-url=$(echo '${baseUrl}' | base64 -w 0)
    user: root
    sync: false

  prepare-worker-integration:
    - cmd[${nodes.k8sm.master.id}]: tar zcfv - /var/lib/kubelet/worker-data 2>/dev/null | base64 -w 0
    - setGlobals:
        worker_integration: ${response.out}

  apply-worker-config:
    - cmd[${nodes.k8sm.master.id}]: screen -d -m /usr/local/sbin/worker-config -n ${this} -g cp -r ${env.region}
    - cmd[${this}]: |-
        mkdir /var/lib/worker &>/dev/null || rm -rf /var/lib/worker/*
        echo '${globals.worker_integration}' | base64 -d | tar zxv --strip-components=4 -C /var/lib/worker
        screen -d -m /usr/bin/bash -c '/usr/local/sbin/worker-integration.sh &>/var/log/kubernetes/k8s-worker-integration.log'

  init-main-master:
    - if (${nodes.mbl.length:0}):
        cmd[mbl]: |-
          sed -i '/^<\/mappings>.*/i \\t<pair frontend_port="6443" backend_port="6443" description="CPlane balancing" option="tcp-check" params="check fall 3 rise 2">' /etc/haproxy/tcpmaps/mappings.xml
          sed -i 's/^bind :::80/#bind :::80/g' /etc/haproxy/haproxy.cfg
          sed -i '/^daemon$/a stats socket /var/run/haproxy.sock mode 660 level admin' /etc/haproxy/haproxy.cfg
          sed -i '/^daemon$/a stats timeout 2m' /etc/haproxy/haproxy.cfg
          echo '${nodes.k8sm.master.intIP}' > /etc/haproxy/hosts
          jem balancer rebuildCommon
        user: root
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        systemctl restart systemd-journald.service
        entryPoint=$((( ${nodes.mbl.length:0} > 0 )) && echo mbl || echo k8sm)
        sed -i "s/^controlPlaneEndpoint:.*/controlPlaneEndpoint: \"${entryPoint}.${env.domain}:6443\"/g" /etc/kubernetes/custom-kubeadm.yaml
        while true; do [ -f "/tmp/jelastic-init-mark" ] && break; echo "Waiting for cluster initial configuration"; sleep 3; done
        kubeadm init --config /etc/kubernetes/custom-kubeadm.yaml --upload-certs --ignore-preflight-errors=swap,numcpu | tee /var/log/kubeadm-init.log
        mkdir /var/lib/kubelet/worker-data
    - configure-master: ${nodes.k8sm.master.id}
    - cmd[${nodes.k8sm.master.id}]: sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep 'control-plane'
    - setGlobals:
        master_join_cmd: ${response.out}
    - cmd[${nodes.k8sm.master.id}]: sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep -v 'control-plane'
    - setGlobals:
        worker_join_cmd: ${response.out}
    - prepare-worker-integration

  init-slave-masters:
    - script: |
        return {
          result : 0,
          nodes: '${nodes.k8sm.join(id,)}'.replace(/\b${nodes.k8sm.master.id},?\b/, '').replace(/,$/, '').split(','),
          ips: '${nodes.k8sm.join(intIP,)}'.replace(/\b${nodes.k8sm.master.intIP},?\b/, '').replace(/,/g, ' ')
        };
    - setGlobals:
        k8sm-slave-ids: ${response.nodes.join(,)}
        k8sm-slave-ips: ${response.ips}
    - if ('${globals.k8sm-slave-ids}'):
      - forEach(node:response.nodes):
          - cmd[${@node}]: |-
              sleep 5
              while true; do [ -f "/tmp/jelastic-init-mark" ] && break; echo "Waiting for cluster initial configuration"; sleep 3; done
              systemctl daemon-reload > /dev/null 2>&1
              systemctl restart systemd-journald.service
              ${globals.master_join_cmd} --ignore-preflight-errors=swap,numcpu | tee /var/log/kubeadm-join.log
      - configure-master: ${globals.k8sm-slave-ids}
      - add-master-balancer: ${globals.k8sm-slave-ips}

  configure-master:
    cmd[${this}]: |-
      mkdir -p $HOME/.kube
      /usr/bin/cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
      chown root:root $HOME/.kube/config
      systemctl enable kubelet.service
      /usr/local/sbin/k8sm-config -f
      while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
      /usr/local/sbin/master-postconfig.sh

  add-master-balancer:
    - cmd[mbl]: |-
        for item in ${this}; do echo "${item}" >> /etc/haproxy/hosts; done
        jem balancer rebuildCommon
      user: root

  connect-workers:
    - cmd[${this}]: |-
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
        systemctl daemon-reload > /dev/null 2>&1
        systemctl restart systemd-journald.service
        screen -d -m /usr/bin/bash -c '${globals.worker_join_cmd} --ignore-preflight-errors=swap,numcpu 1>/var/log/kubeadm-join.log 2>/var/log/kubeadm-join-error.log'
        systemctl enable kubelet.service

  setup-overlay:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/weave-pack.yaml

  install-nginx:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/nginx/nginx-deployment.yaml
      while [[ $(kubectl -n ingress-nginx get pods -l app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/component=controller -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done

  install-haproxy:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/haproxy/haproxy-deployment.yaml

  install-traefik:
     cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-rbac.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-ds.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-ui.yaml

  install-components:
    - cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/install-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --admin-account=true --metallb=true --metrics-server=true --dashboard=${settings.dashboard:none} --ingress-name=${globals.ingress-dir}

  install-helm-master:
    cmd[${nodes.k8sm.master.id}]: |-
      /usr/local/sbin/helm-install.sh | tee /var/log/kubernetes/k8s-helm-master.log

  install-helm-slave:
    cmd[${this}]: screen -d -m /usr/bin/bash -c '/usr/local/sbin/helm-install.sh &>/var/log/kubernetes/k8s-helm-slave.log'

  generate-admin-token:
    - cmd[${nodes.k8sm.master.id}]: |-
        while true; do token_name=$(kubectl -n kube-system get secret | grep fulladmin | awk '{print $1}'); [ -n "$token_name" ] && break; sleep 5; done
        kubectl -n kube-system describe secret "${token_name}" | grep 'token:' | sed -e's/token:\| //g'
    - setGlobals:
        token: ${response.out}

  deploy:
    - if ('${settings.deploy}' == 'cc'):
        cmd[${nodes.k8sm.master.id}]: |-
          kubectl apply -f ${baseUrl}/addons/helloworld.yaml
          kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/helloworld-ingress.yaml
    - if ('${settings.deploy}' == 'cmd'):
        cmd[${nodes.k8sm.master.id}]: |-
          ${settings.cmd}
    - if ('${settings.deploy}' == 'yml'):
        cmd[${nodes.k8sm.master.id}]: kubectl apply -f ${settings.yml}

  helm-components:
     - init-globals-storage
     - cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/helm-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --nfs-provisioner=${settings.storage:false} --nfs-server=${globals.storage_endpoint} --problem-detector=true

  remove-worker:
    - if ('${this.workerExtIPs}'):
        cmd[${nodes.k8sm.master.id}]: metallb-config -d '${this.workerExtIPs}'
    - cmd[${nodes.k8sm.master.id}]: |-
        /usr/bin/kubectl drain ${this.workerHostname} --ignore-daemonsets --delete-local-data || exit 8;
        /usr/bin/kubectl delete node ${this.workerHostname} || exit 9;

  init-globals-ingress:
    - cmd[${nodes.k8sm.master.id}]: |-
        /usr/bin/kubectl get daemonset traefik-ingress-controller -n kube-system &>/dev/null && echo "traefik" ||:
        /usr/bin/kubectl get deployment nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        /usr/bin/kubectl get daemonset nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        /usr/bin/kubectl get daemonset haproxy-ingress -n ingress-controller &>/dev/null && echo "haproxy" ||:
    - setGlobals:
        ingress-dir: ${response.out}

  init-globals-storage:
    - if (${nodes.storage.length:0} > 1):
        setGlobals:
          storage_endpoint: storage.${env.domain}
    - elif (${nodes.storage.length:0} == 1):
        setGlobals:
          storage_endpoint: ${nodes.storage.master.address}
    - else:
        setGlobals:
          storage_endpoint: ""

  setup-remote-api:
  - log: '${this}'
  - cmd[${nodes.k8sm.master.id}]: |-
      action=$([ "${this}" == "true" ] && echo "apply" || echo "delete")
      kubectl $action -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/api-ingress.yaml
  - if (${settings.api:true}):
      - setGlobals:
          default_api: or [Remote API Endpoint](${env.protocol}://${env.domain}/api/)

  install-monitoring:
  - if (${nodes.storage.length:0} == 0):
      return:
        type: warning
        message: Monitoring components require Storage installed!
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret --namespace kubernetes-monitoring monitoring-grafana &>/dev/null && echo "true" || echo "false"
  - setGlobals:
      monitoring_installed: ${response.out}
  - if ('${globals.monitoring_installed}' == 'false'):
      - init-globals-ingress
      - cmd[${nodes.k8sm.master.id}]: |-
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update
          helm install monitoring-prometheus --create-namespace --namespace kubernetes-monitoring prometheus-community/prometheus --set server.prefixURL=/prometheus --set server.baseURL=/prometheus
          wait-deployment.sh monitoring-prometheus-server kubernetes-monitoring 1 720
          rm -rf /root/grafana && helm fetch grafana/grafana --untar
          for dash_name in "kubernetes-prometeus-dashboard" "kubernetes-rchakra3-dashboard" "kubernetes-vanniekerk-dashboard"; do
            wget "${baseUrl}/addons/monitoring/${dash_name}.json" -O "grafana/dashboards/${dash_name}.json"
          done
          helm install monitoring-grafana --namespace kubernetes-monitoring --set 'grafana\.ini'.server.root_url=${env.url}grafana -f ${baseUrl}/addons/monitoring/jelastic-values.yaml grafana/.
          wait-deployment.sh monitoring-grafana kubernetes-monitoring 1 720
          grafana_secret=$(kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}' | base64 --decode ; echo)
          [ "${globals.ingress-dir}" == "haproxy" ] && crypt_option="-1" || crypt_option="-apr1"
          kubectl create secret generic monitoring-prometheus --from-literal=auth="admin:$(openssl passwd ${crypt_option} ${grafana_secret})" --namespace=kubernetes-monitoring
          kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/prometheus-ingress.yaml
          kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/alert-ingress.yaml
          kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/grafana-ingress.yaml
          for i in {1..10}; do
            sleep 10
            echo "Attempt ${i} of Grafana dashboard parameters setting"
            curl -X POST -f -d "user=admin&password=${grafana_secret}" -c grafana/grafana-jar.txt "http://${env.domain}/grafana/login" || contunue
            dash_id=$(curl -sb grafana/grafana-jar.txt 'http://${env.domain}/grafana/api/search?mode=tree&query=Jelastic' | grep -Po '"id":(\d+)' | awk -F ':' '{print $2}')
            [ "${dash_id}" = "" ] && continue
            curl -X POST -f -b grafana/grafana-jar.txt "http://${env.domain}/grafana/api/user/stars/dashboard/${dash_id}" || continue
            curl -X PUT -f -H 'Content-Type: application/json' -b grafana/grafana-jar.txt -d "{\"homeDashboardId\":${dash_id}}" "http://${env.domain}/grafana/api/org/preferences" && break || continue
          done
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}' | base64 --decode
  - setGlobals:
      grafana_secret: ${response.out}
      monitoring_success: |
          Enter [Prometheus dashboard](${env.url}prometheus/), [Prometheus AlertManager](${env.url}prometheus-alert/)
          and [Grafana dashboard](${env.url}grafana/), using login "admin" and password:

          ```${globals.grafana_secret}```
  - if ('${globals.monitoring_installed}' == 'true'):
      return:
        type: info
        message: ${globals.monitoring_success}
  - if (!${settings.monitoring:false}):
      message.email.send:
            to: "${user.email}"
            subject: Monitoring Tools Successfully Installed in ${env.name}
            body: |-
              Monitoring Tools installed in <b>${env.name}</b> Kubernetes Cluster: <br>
              Prometheus Dashboard - ${env.url}prometheus/<br>
              Prometheus AlertManager - ${env.url}prometheus-alert/ <br>
              Grafana Dashboard - ${env.url}grafana/ <br>
              Credentials - admin / ${globals.grafana_secret}
      return:
        type: info
        message: ${globals.monitoring_success}

  install-jaeger:
  - if (${nodes.storage.length:0} == 0):
      return:
        type: warning
        message: Jaeger components require Storage installed!
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret observability-jaeger-plain --namespace=observability &>/dev/null && echo "true" || echo "false"
  - setGlobals:
      jaeger_installed: ${response.out}
  - if ('${globals.jaeger_installed}' == 'false'):
      - init-globals-ingress
      - cmd[${nodes.k8sm.master.id}]: |-
          kubectl create namespace observability
          kubectl create -f ${baseUrl}/addons/jaeger/jaegertracing.io_jaegers_crd.yaml
          kubectl create -f ${baseUrl}/addons/jaeger/service_account.yaml
          kubectl create -f ${baseUrl}/addons/jaeger/role.yaml
          kubectl create -f ${baseUrl}/addons/jaeger/role_binding.yaml
          kubectl create -f ${baseUrl}/addons/jaeger/operator.yaml
          [ "${globals.ingress-dir}" == "haproxy" ] && crypt_option="-1" || crypt_option="-apr1"
          jaeger_secret=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1)
          kubectl create secret generic observability-jaeger-plain --from-literal=auth="${jaeger_secret}" --namespace=observability
          kubectl create secret generic observability-jaeger --from-literal=auth="admin:$(openssl passwd ${crypt_option} ${jaeger_secret})" --namespace=observability
          kubectl apply -f ${baseUrl}/addons/jaeger/jelastic-jaeger.yaml
          kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/jaeger-ingress.yaml
          wait-deployment.sh jaeger-operator observability 1 720
          wait-deployment.sh jaeger observability 1 720
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret --namespace=observability observability-jaeger-plain -o jsonpath='{.data.auth}' | base64 --decode
  - setGlobals:
      jaeger_secret: ${response.out}
      jaeger_success: |
          Enter [Jaeger dashboard](${env.url}jaeger/), using login "admin" and password:

          ```${globals.jaeger_secret}```
  - if ('${globals.jaeger_installed}' == 'true'):
      return:
        type: info
        message: ${globals.jaeger_success}
  - if (!${settings.jaeger:false}):
      message.email.send:
          to: "${user.email}"
          subject: Jaeger Tracing Tools Successfully Installed in ${env.name}
          body: |-
              Jaeger Tracing Tools installed in <b>${env.name}</b> Kubernetes Cluster: <br>
              Jaeger Dashboard - ${env.url}jaeger/ <br>
              Credentials - admin / ${globals.jaeger_secret}
      return:
        type: info
        message: ${globals.jaeger_success}

  check-health:
      cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/check-install.sh -i=${settings.ingress-controller} -app=${settings.deploy} -dash=${settings.dashboard:none} -m=${settings.monitoring:false} -r=${settings.api} -s=${settings.storage} -j=${settings.jaeger:false} -d=${env.domain} &> /var/log/kubernetes/k8s-health-check.log ||
         echo "> **Note:** Some cluster components have not yet been initialized, and it may take some time for pods to start. If you encounter problems with your cluster, please check K8s logs in /var/log/kubernetes on master node and contact support."
      setGlobals:
         check_message: ${response.out}

addons:

  - id: conf-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
    name: Cluster Configuration
    description: Configure remote API access and install complementary tools
    logo: /images/k8s-config.png
    settings:
      fields:
        - type: displayfield
          caption: Useful info
          hideLabel: true
          markup: Access and manage the cluster remotely via API
        - type: checkbox
          name: api
          caption: Remote API access is enabled
        - type: string
          name: ingress-controller
          inputType: hidden

    buttons:
      - caption: Remote API
        settings: main
        action: addon-remote-api
        loadingText: Updating...
        confirmText: Are you sure?
        successText: Remote API Access was successfully updated!
      - caption: Storage
        action: addon-conf-storage
        confirmText: Cluster Storage will be added if missing. Continue?
        successText: Cluster storage has been succesfully installed!
      - caption: Jaeger
        action: install-jaeger
        confirmText: Jaeger will be installed if missing. Continue?

    actions:
      addon-remote-api:
      - log: '${this.api}'
      - init-globals-ingress
      - setup-remote-api: ${this.api}
      - if (${this.api:true}):
          - setGlobals:
              apiStatusMessage: enabled at ${env.url}api
      - else:
          - setGlobals:
              apiStatusMessage: disabled
      - message.email.send:
          to: "${user.email}"
          subject: Remote API Access Successfully Updated in ${env.name}
          body: Remote API has been ${globals.apiStatusMessage}

      addon-conf-storage:
      - if (${nodes.storage.length:0} > 0):
          return:
            type: info
            message: Cluster Storage is already present!
      - script: |
          var resp = jelastic.env.control.GetEnvInfo('${env.appid}', session),
              nodeGroups = [],
              nodeGroupsKubernetes = [],
              path = "/data",
              nodes = [],
              mounts = [],
              nodeGroup,
              mount,
              env;

          if (resp.result != 0) return resp;

          env = {
            shortdomain : resp.env.shortdomain,
            region      : resp.env.hardwareNodeGroup,
            sslstate    : resp.env.sslstate
          };

          var storageCount = ${nodes.k8sm.length} > 1 ? 3 : 1;

          for (var i = 0, k = resp.nodes; i < k.length; i++) {
            nodeGroup = String(k[i].nodeGroup);
            if (nodeGroups.indexOf(nodeGroup) == -1) {
              nodeGroups.push(nodeGroup);

              if (k[i].nodeType == "kubernetes") {
                nodeGroupsKubernetes.push(nodeGroup);
              }

              nodes.push({
                nodeGroup: nodeGroup
              });
            }
          }

          nodes.push({
            count: storageCount,
            nodeType: "storage",
            cloudlets: 8,
            displayName: "Storage",
            nodeGroup: "storage",
            cluster: storageCount > 1
          });

          var res = jelastic.env.control.ChangeTopology('${env.appid}', session, '${env.appid}', toJSON(env), toJSON(nodes));

          if (res.result != 0) return res;

          res = jelastic.env.control.AddContainerVolumeByGroup('${env.name}', session, 'storage', path);

          if (res.result != 0) return res;

          for (var i = 0; i < nodeGroupsKubernetes.length; i++) {
              mount = {
                method: 'jelastic.env.file.AddMountPointByGroup',
                envName: '${env.name}',
                session: session,
                nodeGroup: nodeGroupsKubernetes[i],
                path: path,
                sourcePath: path,
                readOnly: false,
                sourceNodeId: '${nodes.storage.master.id}'
              };

              if (storageCount > 1) {
                mount.sourceAddressType = "NODE_GROUP"
              }

            mounts.push(mount);
          }

          return {result: 0, onAfterReturn: { api: mounts } }

      - init-globals-storage
      - if (!'${globals.storage_endpoint}'):
          return:
            type: error
            message: Cluster Storage wasn't installed!
      - cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/helm-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --nfs-provisioner=true --nfs-server=${globals.storage_endpoint}
      - set-scaling-parameters

  - id: monitor-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
    name: Cluster Monitoring
    description: Install cluster monitoring components (Prometheus and Grafana)
    logo: /images/k8s-monitor.png

    buttons:
      - caption: Install Monitoring Tools
        action: install-monitoring
        confirmText: Monitoring tools will be installed if missing. Continue?

  - id: upgrade-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
    name: Cluster Upgrade
    description: Upgrade Kubernetes cluster to a newer version
    logo: /images/k8s-upgrade.png

    buttons:
      - caption: Start Cluster Upgrade
        action: addon-upgrade-init
        loadingText: Updating...
        confirmText: Do you want to upgrade Kubernetes Cluster?
        successText: Kubernetes Cluster has been successfully upgraded!

    actions:
      addon-upgrade-init:
      - forEach(node:env.nodes):
          if ('${@node.nodeGroup}' == 'cp' || '${@node.nodeGroup}' == 'k8sm'):
            if ('${nodes.k8sm.master.version}' != '${@node.version}'):
              return:
                type: warning
                message: Cluster components have different Kubernetes version! Please contact support before upgrade.
      - script: |
          function compareVersions(a, b) {
            a = a.replace("v", "").split("."); b = b.replace("v", "").split(".");
            for (var i = 0, l = Math.max(a.length, b.length), x, y; i < l; i++) {x = parseInt(a[i], 10) || 0; y = parseInt(b[i], 10) || 0; if (x != y) return x > y ? 1 : -1 }
            return 0;
          }

          var envName = "${env.envName}", nodeId = "${nodes.k8sm.master.id}";
          var resp = jelastic.env.control.GetNodeInfo(envName, session, nodeId);
          if (resp.result != 0) return resp;
          var version = resp.node.version;
          var image = resp.node.name;
          resp = jelastic.env.control.GetContainerNodeTags(envName, session, nodeId);
          if (resp.result != 0) return resp;

          var tags = resp.object;
          tags.sort(compareVersions);
          var upgrades = [];
          var check_version = version;
          var major_version = version.substr(0, version.lastIndexOf("."));

          for (var i = 0; i < tags.length; i++) {
            var major_tag = tags[i].substr(0, tags[i].lastIndexOf("."));
            if (compareVersions(major_tag, major_version) > 0) {
              check_version = tags[i];
              upgrades.push(check_version);
              major_version = major_tag;
            }
          }

          var last_version = tags.pop();
          if (compareVersions(last_version, check_version) > 0) upgrades.push(last_version);
          var message = "Current version " + version + " is the latest. No upgrades are available.";
          if (upgrades.length) {
            upgrades.sort(compareVersions);
            var next = upgrades.shift();
            var baseUrl = "${baseUrl}".split("/"); baseUrl.pop(); baseUrl = baseUrl.join("/");
            var url = baseUrl+"/"+next+"/addons/upgrade.jps";
            var huc = new java.net.URL(url).openConnection();
            huc.setRequestMethod("HEAD");
            var code = huc.getResponseCode();
            if (code == 200){
              return {result:0, onAfterReturn:{"addon-upgrade-start":{current:version, next:next, avail:upgrades.join(", "), jps: url}}};
            } else {
              message = "The next version is " + next + ". However, automated upgrade procedure is not available yet. Please check it later, or contact support team if upgrade is required urgently.";
              return {result:"info", message:message};
            }
          } else {
            return {result:"info", message:message};
          }

      addon-upgrade-start:
        jps: ${this.jps}
        envName: ${env.envName}
        current: ${this.current}
        version: ${this.next}
        avail: ${this.avail}
        upgradeScript: |
          jelastic.marketplace.jps.Install({ envName: envName, session: session, jps: jps, settings: { version: version, avail: avail } });
          return { result: 0 };
        script: |
          var envName = '${env.envName}',
          scriptName = envName + '-k8s-upgrade';
          jelastic.dev.scripting.DeleteScript(scriptName);
          resp = jelastic.dev.scripting.CreateScript(scriptName, "js", upgradeScript);
          if (resp.result != 0) return resp;
          java.lang.Thread.sleep(1000);
          jelastic.dev.scripting.Build(scriptName);
          resp = jelastic.utils.scheduler.AddTask({ script: scriptName, trigger: "once_delay:1000", description: "Upgrade Kubernetes", params: { envName: envName, jps: jps, version: version, avail: avail } });
          if (resp.result != 0) return resp;
          java.lang.Thread.sleep(3000);
          return { type: "info", message: "Kubernetes Cluster " + current + " upgrade to " + version + " has been started.\n\nThe update process may take several minutes depending on number of nodes and deployed services." };

  - id: gitlab-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
    name: GitLab Integration
    description: Add Kubernetes GitLab integrations
    logo: /images/k8s-gitlab.png
    settings:
      fields:
        - type: displayfield
          hideLabel: true
          markup: This addon provides Kubernetes and GitLab integration. Please select the Gitlab environment from the list.
        - type: displayfield
          hideLabel: true
        - type: envlist
          name: envlist
          valueField: shortdomain
          caption: GitLab environment

    buttons:
      - caption: Configure
        settings: gitlab
        action: addon-gitlab-config
        loadingText: Configuration...
        confirmText: Are you sure?
        successText: GitLab integration successfully configured!
      - caption: Remove Integration
        action: addon-gitlab-remove
        confirmText: Any existing Kubernetes and GitLab integration will be removed. Continue?
        successText: GitLab integration successfully removed!

    actions:
      addon-gitlab-config:
      - log: '${settings.envlist}'
      - cmd[${nodes.k8sm.master.id}]: kubectl get configmaps -n gitlab-managed-apps gitlab-configuration &>/dev/null && echo "true" || echo "false"
      - set:
          gitlab_installed: ${response.out}
      - if ('${this.gitlab_installed}' == 'true'):
          return:
            type: info
            message: This cluster already has GitLab integration!
      - env.control.GetEnvInfo:
          envName: ${settings.envlist}
      - set:
          gitlab_domain: ${response.env.domain}
      - env.control.ExecCmdByGroup [cp]:
          envName: ${settings.envlist}
          commandList:
            - command: echo ${ROOT_PASSWORD}
      - set:
          gitlab_pass: ${response.out}
      - env.control.ExecCmdByGroup [cp]:
          envName: ${settings.envlist}
          commandList:
            - command: echo ${HTTPS_PORT}
      - set:
          gitlab_port: ${response.out}
          gitlab_http_endpoint: "https://${this.gitlab_domain}:${this.gitlab_port}"
      - env.control.ExecCmdByGroup [cp]:
          envName: ${settings.envlist}
          commandList:
            - command: echo ${REGISTRY_PORT}
      - set:
          gitlab_reg_port: ${response.out}
      - env.control.ExecCmdByGroup [cp]:
          envName: ${settings.envlist}
          commandList:
            - command: cat /srv/docker/gitlab/certs/ca.crt | base64 -w 0
      - set:
          gitlab_ca_instance: ${response.out}
      - if ('${this.gitlab_pass}' == '' || '${this.gitlab_port}' == ''):
          return:
            type: warning
            message: Cannot determine GitLab credentials!
      - cmd[${nodes.k8sm.master.id}]: kubectl apply -f ${baseUrl}/addons/gitlab/gitlab-service-account.yaml
      - cmd[${nodes.k8sm.master.id}]: kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep gitlab-admin | awk '{print $1}') | grep '^token:' | awk '{print $2}'
      - set:
          gitlab_token: ${response.out}
      - cmd[${nodes.k8sm.master.id}]: kubectl cluster-info | sed -r "s/\x1B\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]//g" | grep 'Kubernetes master' | awk '/http/ {print $NF}'
      - set:
          gitlab_api_url: ${response.out}
      - cmd[${nodes.k8sm.master.id}]: kubectl get secret $(kubectl get secrets | grep default-token | cut -d " " -f 1) -o jsonpath="{['data']['ca\.crt']}" | base64 --decode
      - set:
          gitlab_ca_cert: ${response.out}
      - cmd[${nodes.k8sm.master.id}]: |-
          # Login
          page_content=$(curl -sk -c gitlab-jar.txt  '${this.gitlab_http_endpoint}/users/sign_in')
          csrf_token=$(echo ${page_content} | perl -ne 'print "$1\n" if /new_user.*?authenticity_token"[[:blank:]]value="(.+?)"/' | sed -n 1p)
          echo "Sign-in CSRF token: ${csrf_token}"
          [ -n "${csrf_token}" ] || exit 1
          curl -X POST -sk -b gitlab-jar.txt -c gitlab-jar.txt -f -d "user[login]=root&user[password]=${this.gitlab_pass}" --data-urlencode "authenticity_token=${csrf_token}" '${this.gitlab_http_endpoint}/users/sign_in'
      - cmd[${nodes.k8sm.master.id}]: |-
          # Cluster
          page_content=$(curl -sk -b gitlab-jar.txt -c gitlab-jar.txt  '${this.gitlab_http_endpoint}/admin/clusters')
          echo ${page_content} | perl -ne 'print "$1\n" if /data-qa-cluster-name="Kubernetes"[[:blank:]]href="\/admin\/clusters\/(.+?)"/' | sed -n 1p
      - if ('${response.out}'):
          return:
            type: warning
            message: GitLab instance already has active integration!
      - cmd[${nodes.k8sm.master.id}]: |-
          # Settings
          page_content=$(curl -sk -b gitlab-jar.txt -c gitlab-jar.txt '${this.gitlab_http_endpoint}/admin/application_settings/network')
          csrf_token=$(echo ${page_content} | perl -ne 'print "$1\n" if /meta[[:blank:]]name="csrf-token"[[:blank:]]content="(.+?)"/' | sed -n 1p)
          echo "Application settings CSRF token: ${csrf_token}"
          [ -n "${csrf_token}" ] || exit 2
          curl -X POST -sk -b gitlab-jar.txt -c gitlab-jar.txt -f -d "_method=patch&application_setting[allow_local_requests_from_web_hooks_and_services]=1&application_setting[allow_local_requests_from_system_hooks]=1&application_setting[dns_rebinding_protection_enabled]=1" --data-urlencode "authenticity_token=${csrf_token}" '${this.gitlab_http_endpoint}/admin/application_settings/network' || exit 2
          curl -sk -b gitlab-jar.txt -c gitlab-jar.txt -f '${this.gitlab_http_endpoint}/admin/application_settings/network' &>/dev/null
      - cmd[${nodes.k8sm.master.id}]: |-
          # Cluster
          for i in {1..5}; do
            sleep 10
            echo "Attempt ${i} of GitLab Kubernetes cluster creation"
            page_content=$(curl -sk -b gitlab-jar.txt -c gitlab-jar.txt  '${this.gitlab_http_endpoint}/admin/clusters/new')
            csrf_token=$(echo ${page_content} | perl -ne 'print "$1\n" if /new_cluster.*?authenticity_token"[[:blank:]]value="(.+?)"/' | sed -n 1p)
            echo "Add cluster CSRF token: ${csrf_token}"
            [ -n "${csrf_token}" ] || continue
            page_content=$(curl -X POST -sk -b gitlab-jar.txt -c gitlab-jar.txt -d "cluster[name]=Kubernetes&cluster[platform_kubernetes_attributes][authorization_type]=rbac&cluster[managed]=1" --data-urlencode "cluster[platform_kubernetes_attributes][api_url]=${this.gitlab_api_url}" --data-urlencode "cluster[platform_kubernetes_attributes][ca_cert]=${this.gitlab_ca_cert}" --data-urlencode "cluster[platform_kubernetes_attributes][token]=${this.gitlab_token}" --data-urlencode "authenticity_token=${csrf_token}" '${this.gitlab_http_endpoint}/admin/clusters/create_user')
            cluster_url=$(echo ${page_content} | perl -ne 'print "$1\n" if /You[[:blank:]]are[[:blank:]]being[[:blank:]]\<a[[:blank:]]href="(.+?)"\>redirected/' | sed -n 1p)
            echo "Kubernetes GitLab cluster: ${cluster_url}"
            [ -n "${cluster_url}" ] && break || continue
          done
          echo "${page_content}"
          [ -n "${cluster_url}" ] || exit 4
          cluster_id=$(echo ${cluster_url} | perl -ne 'print "$1\n" if /\/admin\/clusters\/(\d+)$/')
          [ -n "${cluster_id}" ] || exit 4
          kubectl create ns gitlab-managed-apps
          kubectl create configmap gitlab-configuration --from-literal cluster_url="${cluster_url}" --from-literal cluster_id="${cluster_id}" --from-literal cluster_env="${settings.envlist}" -n gitlab-managed-apps
      - cmd[${nodes.k8sm.master.id}]: kubectl get configmaps -n gitlab-managed-apps gitlab-configuration -o jsonpath='{.data.cluster_url}'
      - set:
          gitlab_cluster_url: ${response.out}
      - cmd[${nodes.k8sm.master.id}]: |-
          # Runner
          page_content=$(curl -sk -b gitlab-jar.txt -c gitlab-jar.txt "${this.gitlab_cluster_url}")
          csrf_token=$(echo ${page_content} | perl -ne 'print "$1\n" if /csrf-param.*?csrf-token"[[:blank:]]content="(.+?)"/' | sed -n 1p)
          echo "Runner CSRF token: ${csrf_token}"
          [ -n "${csrf_token}" ] || exit 5
          curl -X POST -sk -b gitlab-jar.txt -c gitlab-jar.txt -f -H "X-CSRF-Token: ${csrf_token}" "${this.gitlab_cluster_url}/applications/runner" || exit 5
          wait-deployment.sh runner-gitlab-runner gitlab-managed-apps 1 720
      - cmd[${nodes.k8sm.master.id}]: |-
          # Integration
          echo '${this.gitlab_ca_instance}' | base64 -d > /var/lib/kubelet/worker-data/gitlab-cacert.crt
          echo 'GITLAB_REGISTRY="${this.gitlab_domain}:${this.gitlab_reg_port}"' > /var/lib/kubelet/worker-data/gitlab-integration.conf
          wget -nv ${baseUrl}/addons/gitlab/gitlab-integration.sh -O /var/lib/kubelet/worker-data/gitlab-integration.sh
          chmod +x /var/lib/kubelet/worker-data/gitlab-integration.sh
      - prepare-worker-integration
      - apply-worker-integration: cp

      addon-gitlab-remove:
      - cmd[${nodes.k8sm.master.id}]: kubectl get ns gitlab-managed-apps &>/dev/null && echo "true" || echo "false"
      - setGlobals:
          gitlab_installed: ${response.out}
      - if ('${globals.gitlab_installed}' == 'false'):
          return:
            type: info
            message: This cluster has no active GitLab integration!
      - cmd[${nodes.k8sm.master.id}]: kubectl get configmaps -n gitlab-managed-apps gitlab-configuration -o jsonpath='{.data.cluster_env}'
      - set:
          gitlab_cluster_env: ${response.out}
      - cmd[${nodes.k8sm.master.id}]: kubectl get configmaps -n gitlab-managed-apps gitlab-configuration -o jsonpath='{.data.cluster_id}'
      - set:
          gitlab_cluster_id: ${response.out}
      - env.control.GetEnvInfo:
          envName: ${this.gitlab_cluster_env}
      - set:
          gitlab_domain: ${response.env.domain}
      - env.control.ExecCmdByGroup [cp]:
          envName: ${this.gitlab_cluster_env}
          commandList:
            - command: echo ${ROOT_PASSWORD}
      - set:
          gitlab_pass: ${response.out}
      - env.control.ExecCmdByGroup [cp]:
          envName: ${this.gitlab_cluster_env}
          commandList:
            - command: echo ${HTTPS_PORT}
      - set:
          gitlab_port: ${response.out}
          gitlab_http_endpoint: "https://${this.gitlab_domain}:${this.gitlab_port}"
      - if ('${this.gitlab_pass}' == '' || '${this.gitlab_port}' == ''):
          return:
            type: warning
            message: Cannot determine GitLab credentials!
      - cmd[${nodes.k8sm.master.id}]: |-
          # Login
          page_content=$(curl -sk -c gitlab-jar.txt  '${this.gitlab_http_endpoint}/users/sign_in')
          csrf_token=$(echo ${page_content} | perl -ne 'print "$1\n" if /new_user.*?authenticity_token"[[:blank:]]value="(.+?)"/' | sed -n 1p)
          echo "Sign-in CSRF token: ${csrf_token}"
          [ -n "${csrf_token}" ] || exit 1
          curl -X POST -sk -b gitlab-jar.txt -c gitlab-jar.txt -f -d "user[login]=root&user[password]=${this.gitlab_pass}" --data-urlencode "authenticity_token=${csrf_token}" '${this.gitlab_http_endpoint}/users/sign_in'
      - cmd[${nodes.k8sm.master.id}]: |-
          # Runner
          page_content=$(curl -sk -b gitlab-jar.txt -c gitlab-jar.txt  '${this.gitlab_http_endpoint}/admin/clusters/${this.gitlab_cluster_id}')
          csrf_token=$(echo ${page_content} | perl -ne 'print "$1\n" if /csrf-param.*?csrf-token"[[:blank:]]content="(.+?)"/' | sed -n 1p)
          echo "Remove runner CSRF token: ${csrf_token}"
          [ -n "${csrf_token}" ] || exit 5
          curl -X DELETE -sk -b gitlab-jar.txt -c gitlab-jar.txt -H "X-CSRF-Token: ${csrf_token}" '${this.gitlab_http_endpoint}/admin/clusters/${this.gitlab_cluster_id}/applications/runner'
      - cmd[${nodes.k8sm.master.id}]: |-
          while true; do kubectl -n gitlab-managed-apps get deploy runner-gitlab-runner --no-headers && sleep 5 || break; done
      - cmd[${nodes.k8sm.master.id}]: |-
          # Cluster
          sleep 10
          page_content=$(curl -sk -b gitlab-jar.txt -c gitlab-jar.txt  '${this.gitlab_http_endpoint}/admin/clusters/${this.gitlab_cluster_id}')
          csrf_token=$(echo ${page_content} | perl -ne 'print "$1\n" if /csrf-param.*?csrf-token"[[:blank:]]content="(.+?)"/' | sed -n 1p)
          echo "Remove cluster CSRF token: ${csrf_token}"
          [ -n "${csrf_token}" ] || exit 5
          curl -X POST -sk -b gitlab-jar.txt -c gitlab-jar.txt -d "_method=delete&no_cleanup=true&confirm_cluster_name_input=Kubernetes" --data-urlencode "authenticity_token=${csrf_token}" '${this.gitlab_http_endpoint}/admin/clusters/${this.gitlab_cluster_id}'
      - cmd[${nodes.k8sm.master.id}]: |-
          sleep 10
          rm -f /var/lib/kubelet/worker-data/gitlab-*
          kubectl delete ns gitlab-managed-apps
          kubectl delete -f ${baseUrl}/addons/gitlab/gitlab-service-account.yaml
      - cmd[cp]: |-
          wget -nv ${baseUrl}/addons/gitlab/gitlab-integration-remove.sh -O /var/lib/worker/gitlab-integration-remove.sh
          chmod +x /var/lib/worker/gitlab-integration-remove.sh
          /var/lib/worker/gitlab-integration-remove.sh | tee -a /var/log/kubernetes/k8s-worker-integration.log
          rm -f /var/lib/worker/gitlab-*

  - id: certman-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
    name: Certificate Manager
    description: |
      Kubernetes SSL Certificate Manager allows to bind custom domain names
      to the cluster and manage SSL certificates
    logo: /images/k8s-cert.png
    settings:
      fields:
        - type: displayfield
          hideLabel: true
          markup: Please specify an external DNS name to assign the certificate with.
        - type: displayfield
          hideLabel: true
        - type: string
          caption: External domain
          name: certificate_domain
    buttons:
      - caption: Install Certificate Manager
        settings: certman
        action: addon-certman-config
        loadingText: Installing...
        successText: Certficate manager has been successfully installed!

    actions:
      addon-certman-config:
      - cmd[${nodes.k8sm.master.id}]: kubectl -n cert-manager get deployment cert-manager &>/dev/null && echo "true" || echo "false"
      - if ('${response.out}' == 'true'):
          return:
            type: warning
            message: Kubernetes Certificate Manager is already configured! Please use Kubernetes Cluster settings!
      - cmd[${nodes.k8sm.master.id}]: |-
          certmanager_version=1.0.4
          helm repo add jetstack https://charts.jetstack.io
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm install cert-manager --create-namespace --namespace cert-manager --version v${certmanager_version} jetstack/cert-manager --set installCRDs=true
          wait-deployment.sh cert-manager cert-manager 1 720
          helm install cert-manager-nginx ingress-nginx/ingress-nginx --namespace cert-manager --set controller.ingressClass=nginx-cert
          wait-deployment.sh cert-manager-nginx-ingress-nginx-controller cert-manager 1 720
          for issuer_name in "le-production-issuer" "le-staging-issuer"; do
            wget "${baseUrl}/addons/cert-manager/${issuer_name}.yaml" -O "/tmp/${issuer_name}.yaml"
            sed -i 's/user@example\.com/${user.email}/g' "/tmp/${issuer_name}.yaml"
            kubectl create -f "/tmp/${issuer_name}.yaml"
          done
          kubectl apply -f ${baseUrl}/addons/cert-manager/helloworld-service.yaml
          wget "${baseUrl}/addons/cert-manager/helloworld-ingress.yaml" -O /tmp/helloworld-ingress.yaml
          sed -i 's/example\.com/${settings.certificate_domain}/g' /tmp/helloworld-ingress.yaml
          kubectl apply -f /tmp/helloworld-ingress.yaml

  - id: rancher-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
    name: Rancher Installer
    description: Rancher Management Platform
    logo: /images/k8s-rancher.png

    buttons:
      - caption: Rancher Platform Installation
        action: addon-rancher
        loadingText: Installing...
        confirmText: Do you want to install Rancher Management Platform?

    actions:
      addon-rancher:
      - cmd[${nodes.k8sm.master.id}]: kubectl -n cattle-system get deployment rancher &>/dev/null && echo "true" || echo "false"
      - if ('${response.out}' == 'true'):
          return:
            type: info
            message: Rancher Management Platform is already installed!
      - init-globals-ingress
      - if ('${globals.ingress-dir}' != 'nginx'):
          return:
            type: warning
            message: Rancher Platforms requires 'nginx' ingress controller! The current ingress controller is '${globals.ingress-dir}'
      - cmd[${nodes.k8sm.master.id}]: |-
          kubectl delete -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/helloworld-ingress.yaml ||:
          kubectl delete -f ${baseUrl}/addons/helloworld.yaml ||:
      - cmd[${nodes.k8sm.master.id}]: |-
          http_code=$(curl -Lks -o /dev/null -w "%{http_code}" "${env.url}")
          ret_code=$?
          if [ ${ret_code} -ne 0 ]; then
            echo "An error occurred while accessing the [endpoint](${env.url})"
          elif [ ${http_code} -eq 200 ]; then
            echo "The [endpoint](${env.url}) is already taken"
          elif [ ${http_code} -ne 404 ]; then
            echo "The [endpoint](${env.url}) is present and answered with HTTP code ${http_code}"
          fi
      - if ('${response.out}'):
          return:
            type: warning
            message: Cannot deploy Rancher UI! ${response.out}.
      - cmd[${nodes.k8sm.master.id}]: |-
          helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
          helm repo update
          helm install rancher rancher-stable/rancher --create-namespace --namespace cattle-system --set tls=external
          kubectl -n cattle-system rollout status deploy/rancher
      - message.email.send:
          to: "${user.email}"
          subject: Rancher Platform Successfully Installed in ${env.name}
          body: |-
            Rancher Management Platform installed in <b>${env.name}</b> Kubernetes Cluster: <br>
            Rancher Dashboard - ${env.url}<br>
            Set your login credentials there during the first login.
      - return:
          type: success
          message: |
            Rancher Platform has been successfully installed!

            Enter [Rancher dashboard](${env.url}), and set your login credentials there.

  - id: regcreds-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.18.18
    name: DockerHub Registry Credentials
    description: |
      Leverage DockerHub images pull rate limits: assign DockerHub user credentials to Kubernetes deployments cluster-wide
    logo: /images/k8s-regcreds.png
    settings:
      fields:
        - type: displayfield
          hideLabel: true
          markup: Please specify DockerHub user's valid credentials in the fields below. All previously stored DockerHub user credentials (if any) will be overwritten!
        - type: displayfield
          hideLabel: true
        - type: string
          required: true
          regex: "^((?!\\s).)*$"
          regexText: Incorrect username
          caption: Username
          name: creds_username
        - type: string
          required: true
          inputType: password
          caption: Password
          name: creds_password
        - type: string
          required: true
          vtype: email
          caption: E-mail
          name: creds_email
    buttons:
      - caption: DockerHub Credentials
        settings: regcreds
        action: addon-regcreds-config
        loadingText: Setting up...
        successText: DockerHub registry user credentials have been successfully set!

    actions:
      addon-regcreds-config:
      - cmd[${nodes.k8sm.master.id}]: |-
          kubectl apply -f ${baseUrl}/addons/registry-creds/registry-creds-system.yaml
          wait-deployment.sh registry-creds-registry-creds-controller registry-creds-system 1 600
          kubectl -n kube-system delete secret dockerhub-credentials-secret
          kubectl delete -f ${baseUrl}/addons/registry-creds/dockerhub-secret.yaml
          kubectl -n kube-system create secret docker-registry dockerhub-credentials-secret --docker-username='${settings.creds_username}' --docker-password='${settings.creds_password}' --docker-email='${settings.creds_email}'
          kubectl apply -f ${baseUrl}/addons/registry-creds/dockerhub-secret.yaml

success: |
  ${globals.default_success:}
  ${globals.monitoring_success:}
  ${globals.jaeger_success:}
  ${globals.check_message:}

  Press **Open in Browser** to view a default web page of your application.
  To bind a custom domain name with your Kubernetes cluster please refer to the steps described in Jelastic [documentation](https://docs.jelastic.com/custom-domains).
