type: install
version: 1.5
id: kubernetes-release
baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.9
description:
  text: /text/description-kube.md
  short: Kubernetes cluster with automated scaling & cost efficient pay-per-use pricing for running cloud-native microservices.
categories:
  - apps/clusters
  - apps/dev-and-admin-tools

logo: /images/k8s-logo.png
name: Kubernetes Cluster v1.29.9
targetRegions:
  type: vz7

ssl: true
onBeforeInit: /scripts/beforeinit.js
onBeforeInstall: /scripts/beforeinstall.js

nodes: definedInOnBeforeInstall

skipNodeEmails: true

globals:
    dashboardUrl:  https://${env.domain}/kubernetes-dashboard/

onInstall:
  - set-scaling-parameters
  - prepare-instances
  - init-main-cplane
  - init-secondary-cplanes
  - connect-workers: cp
  - apply-worker-config:
      nodes: ${nodes.cp.join(id,)}
      group: cp
  - setup-overlay
  - if ('${settings.ingress-controller}' == 'Nginx'):
      - setGlobals:
          ingress-dir: nginx
      - install-nginx
  - elif ('${settings.ingress-controller}' == 'HAProxy'):
      - setGlobals:
          ingress-dir: haproxy
      - install-haproxy
  - else:
      - setGlobals:
          ingress-dir: traefik
      - install-traefik
  - install-components
  - install-helm-main
  - if ('${globals.k8sm-secondary-ids}'):
      install-helm-secondary: ${globals.k8sm-secondary-ids}
  - generate-admin-token
  - helm-components
  - deploy

  - if (${settings.api:true}):
      - setup-remote-api: true

  - if (${settings.monitoring:false}):
        - install-monitoring:
            source: main

  - if (${settings.jaeger:false}):
        - install-jaeger:
            source: main

  - if ('${env.protocol}' == 'http'):
      - api: env.control.AddEndpoint
        nodeId: ${nodes.cp.master.id}
        privatePort: 30777
        protocol: TCP
        name: Dashboard Self-Signed HTTPS
      - setGlobals:
          dashboardUrl: https://node${nodes.cp.master.id}-${env.domain}:${response.object.publicPort}/

  - setGlobals:
        default_success: |
             **Cluster URL:** <${env.protocol}://${env.domain}>

             Enter [Kubernetes dashboard](${globals.dashboardUrl}) ${globals.default_api:} using the Access Token:
             <pre style="height: 64px;overflow: auto;"><code style="white-space: normal;">${globals.token}</code></pre>
  - check-health

onAfterScaleOut:
  - if ('${event.response.nodes.nodeType}' == 'kubernetes' && '${event.response.nodes.nodeGroup}' != 'k8sm'):
      - if (${nodes.storage.length:0} > 0):
          script: |
            var resp = jelastic.env.file.GetMountPoints({envName: '${env.appid}', session: session, nodeGroup: '${event.response.nodes.nodeGroup}'}),
              path = "/data",
              mount;

            if (resp.result != 0) return resp;

            for (var i = 0, k = resp.array; i < k.length; i++) {
              if (path == String(k[i].path)) {
                return { result: 0 };
              }
            }

            mount = {
              method: 'jelastic.env.file.AddMountPointByGroup',
              envName: '${env.name}',
              session: session,
              nodeGroup: '${event.response.nodes.nodeGroup}',
              path: path,
              sourcePath: path,
              readOnly: false,
              sourceNodeId: '${nodes.storage.master.id}'
            };

            if (${nodes.storage.length:0} > 1) {
              mount.sourceAddressType = "NODE_GROUP"
            }

            return {result: 0, onAfterReturn: { api: mount } }

      - env.control.ApplyNodeGroupData[${event.response.nodes.nodeGroup}]:
          data:
            validation:
              scalingMode: "stateless"
      - cmd[${nodes.k8sm.master.id}]: |-
          token_age=$(expr $(date +%s) - $(stat /var/log/kubeadm-init.log -c %Y))
          [ ${token_age} -lt $((20*60*60)) ] && { sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep -v 'control-plane'; } || { kubeadm token create --print-join-command; }
      - setGlobals:
          worker_join_cmd: ${response.out}
      - set:
          nodes: ${event.response.nodes.join(id,)}
          ips: ${event.response.nodes.join(extIPs,)}
      - cmd [${this.nodes}]: init-instance.sh --type=worker --initial=true --base-url=$(echo '${baseUrl}' | base64 -w 0)
      - connect-workers: ${this.nodes}
      - apply-worker-config:
          nodes: ${this.nodes}
          group: ${event.response.nodes.nodeGroup}
      - if ('${this.ips}'):
          cmd[${nodes.k8sm.master.id}]: metallb-config -a '${this.ips}'

onBeforeScaleIn:
  - if ('${event.response.nodes.nodeType}' == 'kubernetes' && '${event.response.nodes.nodeGroup}' != 'k8sm'):
      forEach(event.response.nodes):
        remove-worker:
          workerHostname: node${@i.id}-${env.domain}
          workerExtIPs: ${@i.extIPs.join(,)}

onBeforeClone:
  stopEvent:
    type: warning
    message: Kubernetes Cluster cloning is not supported yet!

onBeforeMigrate:
  stopEvent:
    type: warning
    message: Kubernetes Cluster migration is not supported!

onBeforeAttachExtIp[k8sm]: block-ip-assignment

onBeforeAttachExtIp[mbl]: block-ip-assignment

onBeforeSetExtIpCount[k8sm]: block-ip-assignment

onBeforeSetExtIpCount[mbl]: block-ip-assignment

onAfterSetExtIpCount:
  - env.control.GetNodeInfo[${event.params.nodeid}]
  - if ('${response.node.nodeType}' == 'kubernetes' && '${response.node.nodeGroup}' != 'k8sm'):
      - set:
          attachIps: ${event.response.attachedIps.join(,)}
          detachIps: ${event.response.detachedIps.join(,)}
      - log: 'attached ips: ${this.attachIps}, detached ips: ${this.detachIps}'
      - cmd[${nodes.k8sm.master.id}]: metallb-config -a '${this.attachIps}' -d '${this.detachIps}'
      - cmd[${event.params.nodeid}]: systemctl restart kube-config.service

onBeforeDetachExtIp:
  - env.control.GetNodeInfo[${event.params.nodeid}]
  - if ('${response.node.nodeType}' == 'kubernetes' && '${response.node.nodeGroup}' != 'k8sm'):
      - set:
          detachIp: ${event.params.ip}
      - log: 'detached ip: ${this.detachIp}'
      - cmd[${nodes.k8sm.master.id}]: metallb-config -d '${this.detachIp}'
      - cmd[${event.params.nodeid}]: systemctl restart kube-config.service

onAfterAttachExtIp:
  - if (${event.response.result:1} == 0):
      - env.control.GetNodeInfo[${event.params.nodeid}]
      - if ('${response.node.nodeType}' == 'kubernetes' && '${response.node.nodeGroup}' != 'k8sm'):
          - set:
              attachIp: ${event.response.object}
          - log: 'attached ip: ${this.attachIp}'
          - cmd[${nodes.k8sm.master.id}]: metallb-config -a '${this.attachIp}'
          - cmd[${event.params.nodeid}]: systemctl restart kube-config.service

onAfterSetCloudletCount:
  - script: |
      var resp = jelastic.env.control.GetEnvInfo('${env.appid}', session);

      if (resp.result != 0 || resp.nodes == null) return { result: 0, isKubernetes: "false" };

      for (var i = 0, k = resp.nodes; i < k.length; i++) {
        nodeGroup = String(k[i].nodeGroup);
        nodeType = String(k[i].nodeType);
        if (nodeGroup == "${event.params.nodeGroup}" && nodeType == "kubernetes") {
          return { result: 0, isKubernetes: "true" }
        }
      }

      return { result: 0, isKubernetes: "false" };

  - if ('${response.isKubernetes}' == 'true'):
      - cmd[${event.params.nodeGroup}]: systemctl restart kubelet.service

actions:
  set-scaling-parameters:
    - env.control.ApplyNodeGroupData[k8sm]:
        data:
          validation:
            minCount: ${nodes.k8sm.length}
            maxCount: ${nodes.k8sm.length}
            scalingMode: "stateless"
    - if (${nodes.storage.length:0} > 0):
        - script: |
            return { result: 0, min: ${nodes.storage.length:0} > 1 ? 3: 1, max: ${nodes.storage.length:0} > 1 ? 7: 1 }
        - env.control.ApplyNodeGroupData[storage]:
            data:
              validation:
                countIncrement: 2
                minCount: ${response.min}
                maxCount: ${response.max}

  block-ip-assignment:
    stopEvent:
      type: warning
      message: Kubernetes service instances shouldn't have external IPs assigned.

  prepare-instances:
    execCmd:
      - nodeGroup: k8sm
        commands: init-instance.sh --type=cplane --initial=true --base-url=$(echo '${baseUrl}' | base64 -w 0)
      - nodeGroup: cp
        commands: init-instance.sh --type=worker --initial=true --base-url=$(echo '${baseUrl}' | base64 -w 0)
    user: root
    sync: false

  apply-worker-integration:
    - cmd[${nodes.k8sm.master.id}]: tar zcfv - /var/lib/kubelet/worker-data 2>/dev/null | base64 -w 0
    - set:
        worker_integration: ${response.out}
    - cmd[${this.nodes}]: |-
        mkdir /var/lib/worker &>/dev/null || rm -rf /var/lib/worker/*
        echo '${this.worker_integration}' | base64 -d | tar zxv --strip-components=4 -C /var/lib/worker
        screen -d -m /usr/bin/bash -c '/usr/local/sbin/worker-integration.sh &>/var/log/kubernetes/k8s-worker-integration.log'

  apply-worker-config:
    - cmd[${nodes.k8sm.master.id}]: screen -d -m /usr/local/sbin/worker-config -n ${this.nodes} -g ${this.group} -r ${env.region}
    - apply-worker-integration:
        nodes: ${this.nodes}

  init-main-cplane:
    - if (${nodes.mbl.length:0}):
        cmd[mbl]: |-
          sed -i '/^<\/mappings>.*/i \\t<pair frontend_port="6443" backend_port="6443" description="CPlane balancing" option="tcp-check" params="check fall 3 rise 2">' /etc/haproxy/tcpmaps/mappings.xml
          sed -i 's/^bind :::80/#bind :::80/g' /etc/haproxy/haproxy.cfg
          sed -i '/^daemon$/a stats socket /var/run/haproxy.sock mode 660 level admin' /etc/haproxy/haproxy.cfg
          sed -i '/^daemon$/a stats timeout 2m' /etc/haproxy/haproxy.cfg
          echo '${nodes.k8sm.master.intIP}' > /etc/haproxy/hosts
          jem balancer rebuildCommon
        user: root
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        systemctl restart systemd-journald.service
        entryPoint=$((( ${nodes.mbl.length:0} > 0 )) && echo mbl || echo k8sm)
        sed -i "s/^controlPlaneEndpoint:.*/controlPlaneEndpoint: \"${entryPoint}.${env.domain}:6443\"/g" /etc/custom-kubeadm.yaml
        while true; do [ -f "/tmp/jelastic-init-mark" ] && break; echo "Waiting for cluster initial configuration"; sleep 3; done
        kubeadm init --config /etc/custom-kubeadm.yaml --upload-certs --ignore-preflight-errors=swap,numcpu | tee /var/log/kubeadm-init.log
        mkdir /var/lib/kubelet/worker-data
    - configure-cplane: ${nodes.k8sm.master.id}
    - cmd[${nodes.k8sm.master.id}]: sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep 'control-plane'
    - setGlobals:
        cplane_join_cmd: ${response.out}
    - cmd[${nodes.k8sm.master.id}]: sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep -v 'control-plane'
    - setGlobals:
        worker_join_cmd: ${response.out}

  init-secondary-cplanes:
    - script: |
        return {
          result : 0,
          nodes: '${nodes.k8sm.join(id,)}'.replace(/\b${nodes.k8sm.master.id},?\b/, '').replace(/,$/, '').split(','),
          ips: '${nodes.k8sm.join(intIP,)}'.replace(/\b${nodes.k8sm.master.intIP},?\b/, '').replace(/,/g, ' ')
        };
    - setGlobals:
        k8sm-secondary-ids: ${response.nodes.join(,)}
        k8sm-secondary-ips: ${response.ips}
    - if ('${globals.k8sm-secondary-ids}'):
      - forEach(node:response.nodes):
          - cmd[${@node}]: |-
              sleep 5
              while true; do [ -f "/tmp/jelastic-init-mark" ] && break; echo "Waiting for cluster initial configuration"; sleep 3; done
              systemctl daemon-reload > /dev/null 2>&1
              systemctl restart systemd-journald.service
              ${globals.cplane_join_cmd} --ignore-preflight-errors=swap,numcpu | tee /var/log/kubeadm-join.log
      - configure-cplane: ${globals.k8sm-secondary-ids}
      - add-cplane-balancer: ${globals.k8sm-secondary-ips}

  configure-cplane:
    cmd[${this}]: |-
      mkdir -p $HOME/.kube
      /usr/bin/cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
      chown root:root $HOME/.kube/config
      systemctl enable kubelet.service
      /usr/local/sbin/k8sm-config -f
      while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
      /usr/local/sbin/cplane-postconfig.sh

  add-cplane-balancer:
    - cmd[mbl]: |-
        for item in ${this}; do echo "${item}" >> /etc/haproxy/hosts; done
        jem balancer rebuildCommon
      user: root

  connect-workers:
    - cmd[${this}]: |-
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
        systemctl daemon-reload > /dev/null 2>&1
        systemctl restart systemd-journald.service
        screen -d -m /usr/bin/bash -c '${globals.worker_join_cmd} --ignore-preflight-errors=swap,numcpu 1>/var/log/kubeadm-join.log 2>/var/log/kubeadm-join-error.log'
        systemctl enable kubelet.service

  setup-overlay:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/weave-pack.yaml

  install-nginx:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/nginx/nginx-deployment.yaml
      while [[ $(kubectl -n ingress-nginx get pods -l app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/component=controller -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done
      while true; do kubectl get -A ValidatingWebhookConfiguration && break; sleep 5; done
      while true; do kubectl -n ingress-nginx get svc ingress-nginx-controller-admission && break; sleep 5; done

  install-haproxy:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/haproxy/haproxy-deployment.yaml
      while [[ $(kubectl -n haproxy-controller get pods -l run=haproxy-ingress -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done
      wait-deployment.sh ingress-default-backend haproxy-controller 1 720

  install-traefik:
     cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-ns.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-crd.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-rbac.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-ds.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-class.yaml
      while [[ $(kubectl -n ingress-traefik get pods -l app.kubernetes.io/name=ingress-traefik,app.kubernetes.io/component=controller -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done

  install-components:
    - cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/install-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --admin-account=true --metallb=true --metrics-server=true --dashboard=${settings.dashboard:none} --ingress-name=${globals.ingress-dir}

  install-helm-main:
    cmd[${nodes.k8sm.master.id}]: |-
      /usr/local/sbin/helm-install.sh | tee /var/log/kubernetes/k8s-helm-main.log

  install-helm-secondary:
    cmd[${this}]: screen -d -m /usr/bin/bash -c '/usr/local/sbin/helm-install.sh &>/var/log/kubernetes/k8s-helm-secondary.log'

  generate-admin-token:
    - cmd[${nodes.k8sm.master.id}]: |-
        while true; do token_name=$(kubectl -n kube-system get secret | grep fulladmin | awk '{print $1}'); [ -n "$token_name" ] && break; sleep 5; done
        kubectl -n kube-system describe secret "${token_name}" | grep 'token:' | sed -e's/token:\| //g'
    - setGlobals:
        token: ${response.out}

  deploy:
    - if ('${globals.ingress-dir}' == 'nginx'):
        cmd[${nodes.k8sm.master.id}]: while true; do kubectl -n ingress-nginx get svc ingress-nginx-controller-admission && break; sleep 5; done
    - elif ('${globals.ingress-dir}' == 'haproxy'):
        cmd[${nodes.k8sm.master.id}]: while true; do kubectl -n haproxy-controller get svc haproxy-ingress && break; sleep 5; done
    - else:
        cmd[${nodes.k8sm.master.id}]: while true; do kubectl -n ingress-traefik get svc traefik-ingress-service && break; sleep 5; done
    - if ('${settings.deploy}' == 'cc'):
        cmd[${nodes.k8sm.master.id}]: |-
          kubectl apply -f ${baseUrl}/addons/helloworld.yaml
          while true; do kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/helloworld-ingress.yaml && break; sleep 5; done
    - elif ('${settings.deploy}' == 'cmd'):
        cmd[${nodes.k8sm.master.id}]: |-
          ${settings.cmd}
    - elif ('${settings.deploy}' == 'yml'):
        cmd[${nodes.k8sm.master.id}]: kubectl apply -f ${settings.yml}
    - else:
        log: Unknown package deploy option '${settings.deploy}', skipped

  helm-components:
     - init-globals-storage
     - cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/helm-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --nfs-provisioner=${settings.storage:false} --nfs-server=${globals.storage_endpoint} --problem-detector=true

  remove-worker:
    - if ('${this.workerExtIPs}'):
        cmd[${nodes.k8sm.master.id}]: metallb-config -d '${this.workerExtIPs}'
    - cmd[${nodes.k8sm.master.id}]: |-
        /usr/bin/kubectl drain ${this.workerHostname} --ignore-daemonsets --delete-emptydir-data || exit 8;
        /usr/bin/kubectl delete node ${this.workerHostname} || exit 9;

  init-globals-ingress:
    - cmd[${nodes.k8sm.master.id}]: |-
        /usr/bin/kubectl get daemonset traefik-ingress-controller -n ingress-traefik &>/dev/null && echo "traefik" ||:
        /usr/bin/kubectl get daemonset nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        /usr/bin/kubectl get daemonset haproxy-ingress -n haproxy-controller &>/dev/null && echo "haproxy" ||:
    - setGlobals:
        ingress-dir: ${response.out}

  init-globals-storage:
    - if (${nodes.storage.length:0} > 1):
        setGlobals:
          storage_endpoint: storage.${env.domain}
    - elif (${nodes.storage.length:0} == 1):
        setGlobals:
          storage_endpoint: ${nodes.storage.master.address}
    - else:
        setGlobals:
          storage_endpoint: ""

  init-globals-workers:
    - script: |
        var resp = jelastic.env.control.GetEnvInfo('${env.appid}', session),
              nodeGroups = [];

        if (resp.result != 0) return resp;

        for (var i = 0, k = resp.nodes; i < k.length; i++) {
           nodeGroup = String(k[i].nodeGroup);
           if (nodeGroups.indexOf(nodeGroup) == -1) {
             if (k[i].nodeType == "kubernetes" && nodeGroup != "k8sm") {
               nodeGroups.push(nodeGroup);
             }
           }
        }

        return { result: 0, onAfterReturn: { setGlobals: { workers: nodeGroups.join() } } }

  setup-remote-api:
  - log: 'api: ${this}'
  - cmd[${nodes.k8sm.master.id}]: |-
      if [ "${this}" == "true" ]; then
        while true; do kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/api-ingress.yaml && break; sleep 5; done
      else
        kubectl delete -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/api-ingress.yaml
      fi
  - if (${settings.api:true}):
      - setGlobals:
          default_api: or [Remote API Endpoint](${env.protocol}://${env.domain}/api/)

  install-monitoring:
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret --namespace kubernetes-monitoring monitoring-grafana &>/dev/null && echo "true" || echo "false"
  - set:
      monitoring_installed: ${response.out}
  - if ('${this.monitoring_installed}' == 'false'):
      - init-globals-ingress
      - cmd[${nodes.k8sm.master.id}]: |-
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update
          helm install monitoring-prometheus --create-namespace --namespace kubernetes-monitoring prometheus-community/prometheus --set server.prefixURL=/prometheus --set server.baseURL=/prometheus
          wait-deployment.sh monitoring-prometheus-server kubernetes-monitoring 1 720
          rm -rf /root/grafana && helm fetch grafana/grafana --untar
          for dash_name in "kubernetes-prometheus-dashboard" "kubernetes-rchakra3-dashboard" "kubernetes-chaiyd-dashboard" "kubernetes-vuxuanlai266-dashboard"; do
            wget "${baseUrl}/addons/monitoring/${dash_name}.json" -O "grafana/dashboards/${dash_name}.json"
          done
          helm install monitoring-grafana --namespace kubernetes-monitoring --set 'grafana\.ini'.server.root_url=${env.url}grafana -f ${baseUrl}/addons/monitoring/jelastic-values.yaml grafana/.
          wait-deployment.sh monitoring-grafana kubernetes-monitoring 1 720
          grafana_secret=$(kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}' | base64 --decode ; echo)
          [ "${globals.ingress-dir}" == "haproxy" ] && crypt_params="admin=$(openssl passwd -1 ${grafana_secret})" || crypt_params="auth=admin:$(openssl passwd -apr1 ${grafana_secret})"
          kubectl create secret generic --namespace=kubernetes-monitoring monitoring-prometheus --from-literal=${crypt_params}
          kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/prometheus-ingress.yaml
          kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/alert-ingress.yaml
          kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/grafana-ingress.yaml
          for i in {1..10}; do
            sleep 10
            echo "Attempt ${i} of Grafana dashboard parameters setting"
            curl -X POST -f -H 'Content-Type: application/json' -d "{\"user\":\"admin\",\"password\":\"${grafana_secret}\"}" -c grafana/grafana-jar.txt "http://${env.domain}/grafana/login" || continue
            dash_id=$(curl -sb grafana/grafana-jar.txt 'http://${env.domain}/grafana/api/search?mode=tree&query=Jelastic' | grep -Po '"id":(\d+)' | awk -F ':' '{print $2}')
            [ "${dash_id}" = "" ] && continue
            curl -X POST -f -b grafana/grafana-jar.txt "http://${env.domain}/grafana/api/user/stars/dashboard/${dash_id}" || continue
            curl -X PUT -f -H 'Content-Type: application/json' -b grafana/grafana-jar.txt -d "{\"homeDashboardId\":${dash_id}}" "http://${env.domain}/grafana/api/org/preferences" && break || continue
          done
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}' | base64 --decode
  - set:
      grafana_secret: ${response.out}
  - setGlobals:
      monitoring_success: |
          Enter [Prometheus dashboard](${env.url}prometheus/), [Prometheus AlertManager](${env.url}prometheus-alert/)
          and [Grafana dashboard](${env.url}grafana/), using login "admin" and password:

          ```${this.grafana_secret}```
  - if ('${this.source}' == 'addon'):
      - if ('${this.monitoring_installed}' == 'false'):
          message.email.send:
            to: "${user.email}"
            subject: Monitoring Tools Successfully Installed in ${env.name}
            body: |-
              Monitoring Tools installed in <b>${env.name}</b> Kubernetes Cluster: <br>
              Prometheus Dashboard - ${env.url}prometheus/<br>
              Prometheus AlertManager - ${env.url}prometheus-alert/ <br>
              Grafana Dashboard - ${env.url}grafana/ <br>
              Credentials - admin / ${this.grafana_secret}

  install-cert-manager:
  - cmd[${nodes.k8sm.master.id}]: |-
      certmanager_version=1.13.1
      helm repo add jetstack https://charts.jetstack.io
      helm repo update
      helm install cert-manager --create-namespace --namespace cert-manager --version v${certmanager_version} jetstack/cert-manager --set installCRDs=true
      wait-deployment.sh cert-manager cert-manager 1 720
      for issuer_name in "le-production-issuer" "le-staging-issuer"; do
        wget "${baseUrl}/addons/cert-manager/${issuer_name}.yaml" -O "/tmp/${issuer_name}.yaml"
        sed -i 's/user@example\.com/${user.email}/g' "/tmp/${issuer_name}.yaml"
        kubectl create -f "/tmp/${issuer_name}.yaml"
      done

  install-jaeger:
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret observability-jaeger-plain --namespace=observability &>/dev/null && echo "true" || echo "false"
  - set:
      jaeger_installed: ${response.out}
  - if ('${this.jaeger_installed}' == 'false'):
      - init-globals-ingress
      - cmd[${nodes.k8sm.master.id}]: kubectl -n cert-manager get deployment cert-manager &>/dev/null && echo "true" || echo "false"
      - if ('${response.out}' == 'false'):
          - install-cert-manager
      - cmd[${nodes.k8sm.master.id}]: |-
          kubectl create namespace observability
          kubectl create --namespace observability -f ${baseUrl}/addons/jaeger/jaeger-operator.yaml
          wait-deployment.sh jaeger-operator observability 1 720
          jaeger_secret=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1)
          [ "${globals.ingress-dir}" == "haproxy" ] && crypt_params="admin=$(openssl passwd -1 ${jaeger_secret})" || crypt_params="auth=admin:$(openssl passwd -apr1 ${jaeger_secret})"
          kubectl create secret generic --namespace=observability observability-jaeger-plain --from-literal=auth="${jaeger_secret}"
          kubectl create secret generic --namespace=observability observability-jaeger --from-literal=${crypt_params}
          kubectl apply -f ${baseUrl}/addons/jaeger/jelastic-jaeger.yaml
          kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/jaeger-ingress.yaml
          wait-deployment.sh jaeger observability 1 720
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret --namespace=observability observability-jaeger-plain -o jsonpath='{.data.auth}' | base64 --decode
  - set:
      jaeger_secret: ${response.out}
  - setGlobals:
      jaeger_success: |
          Enter [Jaeger dashboard](${env.url}jaeger/), using login "admin" and password:

          ```${this.jaeger_secret}```
  - if ('${this.source}' == 'addon'):
      - if ('${this.jaeger_installed}' == 'false'):
          message.email.send:
            to: "${user.email}"
            subject: Jaeger Tracing Tools Successfully Installed in ${env.name}
            body: |-
              Jaeger Tracing Tools installed in <b>${env.name}</b> Kubernetes Cluster: <br>
              Jaeger Dashboard - ${env.url}jaeger/ <br>
              Credentials - admin / ${this.jaeger_secret}

  check-health:
      cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/check-install.sh -i=${settings.ingress-controller} -app=${settings.deploy} -dash=${settings.dashboard:none} -m=${settings.monitoring:false} -r=${settings.api} -s=${settings.storage} -j=${settings.jaeger:false} -d=${env.domain} &> /var/log/kubernetes/k8s-health-check.log ||
         echo "> **Note:** Some cluster components have not yet been initialized, and it may take some time for pods to start. If you encounter problems with your cluster, please check K8s logs in /var/log/kubernetes on control-plane node and contact support."
      setGlobals:
         check_message: ${response.out}

addons:

  - id: conf-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.9
    name: Cluster Configuration
    description: Configure remote API access and install complementary tools
    logo: /images/k8s-config.png
    settings:
      fields:
        - type: displayfield
          caption: Useful info
          hideLabel: true
          markup: Access and manage the cluster remotely via API
        - type: checkbox
          name: api
          caption: Remote API access is enabled
        - type: string
          name: ingress-controller
          inputType: hidden

    buttons:
      - caption: Remote API
        settings: main
        action: addon-remote-api
        loadingText: Updating...
        confirmText: Are you sure?
        successText: Remote API Access was successfully updated!
      - caption: Storage
        action: addon-conf-storage
        confirmText: Cluster Storage will be added if missing. Continue?
        successText: Cluster storage has been succesfully installed!
      - caption: Jaeger
        action: addon-jaeger
        confirmText: Jaeger will be installed if missing. Continue?

    actions:
      addon-remote-api:
      - log: '${this.api}'
      - init-globals-ingress
      - setup-remote-api: ${this.api}
      - if (${this.api:true}):
          - set:
              apiStatusMessage: enabled at ${env.url}api
      - else:
          - set:
              apiStatusMessage: disabled
      - message.email.send:
          to: "${user.email}"
          subject: Remote API Access Successfully Updated in ${env.name}
          body: Remote API has been ${this.apiStatusMessage}

      addon-conf-storage:
      - if (${nodes.storage.length:0} > 0):
          return:
            type: info
            message: Cluster Storage is already present!
      - script: |
          const perEnv = "environment.maxnodescount", perNodeGroup = "environment.maxsamenodescount";
          var quotas = jelastic.billing.account.GetQuotas(perEnv + ";" + perNodeGroup).array;
          var storageCount = ${nodes.k8sm.length} > 1 ? 3 : 1;

          for (var i = 0; i < quotas.length; i++){
            var q = quotas[i], n = toNative(q.quota.name);
            if (n == perEnv && ${env.nodes.length} + storageCount > q.value) {
              return {result:"warning", message:"Environment nodes quota is exhausted!"};
            }
            if (n == perNodeGroup && storageCount > q.value) {
              return {result:"warning", message:"NodeGroup nodes quota is exhausted!"};
            }
          }

          var resp = jelastic.env.control.GetEnvInfo('${env.appid}', session),
              nodeGroups = [],
              nodeGroupsKubernetes = [],
              path = "/data",
              nodes = [],
              mounts = [],
              nodeGroup,
              mount,
              env;

          if (resp.result != 0) return resp;

          env = {
            shortdomain : resp.env.shortdomain,
            region      : resp.env.hardwareNodeGroup,
            sslstate    : resp.env.sslstate
          };

          for (var i = 0, k = resp.nodes; i < k.length; i++) {
            nodeGroup = String(k[i].nodeGroup);
            if (nodeGroups.indexOf(nodeGroup) == -1) {
              nodeGroups.push(nodeGroup);

              if (k[i].nodeType == "kubernetes") {
                nodeGroupsKubernetes.push(nodeGroup);
              }

              nodes.push({
                nodeGroup: nodeGroup
              });
            }
          }

          nodes.push({
            count: storageCount,
            nodeType: "storage",
            cloudlets: 8,
            displayName: "Storage",
            nodeGroup: "storage",
            cluster: storageCount > 1
          });

          var res = jelastic.env.control.ChangeTopology('${env.appid}', session, '${env.appid}', toJSON(env), toJSON(nodes));

          if (res.result != 0) return res;

          res = jelastic.env.control.AddContainerVolumeByGroup('${env.name}', session, 'storage', path);

          if (res.result != 0) return res;

          for (var i = 0; i < nodeGroupsKubernetes.length; i++) {
              mount = {
                method: 'jelastic.env.file.AddMountPointByGroup',
                envName: '${env.name}',
                session: session,
                nodeGroup: nodeGroupsKubernetes[i],
                path: path,
                sourcePath: path,
                readOnly: false,
                sourceNodeId: '${nodes.storage.master.id}'
              };

              if (storageCount > 1) {
                mount.sourceAddressType = "NODE_GROUP"
              }

            mounts.push(mount);
          }

          return {result: 0, onAfterReturn: { api: mounts } }

      - init-globals-storage
      - if (!'${globals.storage_endpoint}'):
          return:
            type: error
            message: Cluster Storage wasn't installed!
      - cmd[${nodes.k8sm.master.id}]: /usr/local/sbin/helm-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --nfs-provisioner=true --nfs-server=${globals.storage_endpoint}
      - set-scaling-parameters

      addon-jaeger:
      - if (${nodes.storage.length:0} == 0):
          return:
            type: warning
            message: Jaeger components require Storage installed!
      - install-jaeger:
          source: addon
      - return:
          type: info
          message: ${globals.jaeger_success}

  - id: monitor-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.9
    name: Cluster Monitoring
    description: Install cluster monitoring components (Prometheus and Grafana)
    logo: /images/k8s-monitor.png

    buttons:
      - caption: Install Monitoring Tools
        action: addon-monitoring
        confirmText: Monitoring tools will be installed if missing. Continue?

    actions:
      addon-monitoring:
      - if (${nodes.storage.length:0} == 0):
          return:
            type: warning
            message: Monitoring components require Storage installed!
      - install-monitoring:
          source: addon
      - return:
          type: info
          message: ${globals.monitoring_success}

  - id: upgrade-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.9
    name: Cluster Upgrade
    description: Upgrade Kubernetes cluster to a newer version
    logo: /images/k8s-upgrade.png
    settings:
      fields:
        - type: displayfield
          hideLabel: true
          markup: |
            Press **Start** to automatically upgrade Kubernetes cluster to a newer version.
            Depending from the current version in the cluster, it will be either a switch to a new major Kubernetes version, or a minor update within the same branch.
      submitUnchanged: true

      onBeforeInit: |
        function compareVersions(a, b) {
          a = a.replace("v", "").split("."); b = b.replace("v", "").split(".");
          for (var i = 0, l = Math.max(a.length, b.length), x, y; i < l; i++) {x = parseInt(a[i], 10) || 0; y = parseInt(b[i], 10) || 0; if (x != y) return x > y ? 1 : -1 }
          return 0;
        }

        var resp = api.system.service.GetVersion();
        if (resp.result != 0) return resp;

        if (compareVersions([resp.version, resp.build].join("."), '6.1.4') >= 0) {
          var envName = "${env.envName}", nodeId = "${nodes.k8sm.master.id}";
          var resp = api.env.control.GetNodeInfo(envName, session, nodeId);
          if (resp.result != 0) return {result:"warning", message:"Cannot retrieve Kubernetes environment settings!"};

          const version = resp.node.version;
          resp = api.env.control.GetContainerNodeTags(envName, session, nodeId);
          if (resp.result != 0) return {result:"warning", message:"Cannot retrieve Kubernetes tags!"};

          var tags = resp.object, upgrades = [];
          tags.sort(compareVersions);
          const major_version = version.substr(0, version.lastIndexOf("."));
          var major_next = major_version;

          for (var i = 0; i < tags.length; i++)
          if (compareVersions(tags[i], version) > 0) {
            var major_tag = tags[i].substr(0, tags[i].lastIndexOf("."));
            if (compareVersions(major_tag, major_version) == 0) {
              upgrades = [ tags[i] ];
            } else if (compareVersions(major_tag, major_next) > 0) {
                upgrades.push(tags[i]);
                major_next = major_tag;
            }
          }
          if (!upgrades.length) return {result:"info", message:"Current version " + version + " is the latest. No upgrades are available."};

          const last_version = tags.pop();
          if (compareVersions(last_version, upgrades[upgrades.length-1]) > 0) {
            upgrades.push(last_version);
          }

          var uplist = {}, updefault;
          for (var i=0; i < upgrades.length; i++) {
            var major_upgrade = upgrades[i].substr(0, upgrades[i].lastIndexOf("."));
            var val = compareVersions(major_upgrade, major_version);
            uplist[upgrades[i]] = upgrades[i];
            updefault = upgrades[i];
            if (val > 0) break;
          }
          if (!Object.keys(uplist).length) return {result:"warning", message:"Cannot determine cluster versions to upgrade!"};

          settings.fields = [{
            type: "displayfield",
            hideLabel: true,
            markup: "Upgrade Kubernetes cluster to a newer version. Depending from the current Kubernetes version in the cluster, a few options may be available."
          }, {
            type: "displayfield",
            hideLabel: true
          }, {
            type: "list",
            name: "uplist",
            values: uplist,
            required: true,
            editable: false,
            default: updefault,
            caption: "Version to upgrade"
          }];

          api.env.control.ApplyNodeGroupData({
            envName: "${env.name}",
            nodeGroup: "k8sm",
            data: {
              version: version,
              avail: upgrades.join(",")
            }
          });

        }
        return settings;

    buttons:
      - caption: Start Cluster Upgrade
        settings: main
        action: addon-upgrade-init
        loadingText: Updating...
        submitButtonText: Start
        successText: Kubernetes Cluster has been successfully upgraded!

    actions:
      addon-upgrade-init:
      - forEach(node:env.nodes):
          if ('${@node.nodeType}' == 'kubernetes'):
            if ('${nodes.k8sm.master.version}' != '${@node.version}'):
              return:
                type: warning
                message: Cluster components have different Kubernetes version! Please contact support before upgrade.
      - script: |
          function compareVersions(a, b) {
            a = a.replace("v", "").split("."); b = b.replace("v", "").split(".");
            for (var i = 0, l = Math.max(a.length, b.length), x, y; i < l; i++) {x = parseInt(a[i], 10) || 0; y = parseInt(b[i], 10) || 0; if (x != y) return x > y ? 1 : -1 }
            return 0;
          }

          function startUpgrade(version, next, upgrades) {
            var baseUrl = "${baseUrl}".split("/"); baseUrl.pop(); baseUrl = baseUrl.join("/");
            var url = baseUrl + "/" + next + "/addons/upgrade.jps";
            var huc = new java.net.URL(url).openConnection();
            huc.setRequestMethod("HEAD");
            var code = huc.getResponseCode();
            if (code == 200){
              return {result:0, onAfterReturn:{"addon-upgrade-start":{current:version, next:next, avail:upgrades.join(", "), jps: url}}};
            } else {
              var message = "The next version is " + next + ". However, automated upgrade procedure is not available yet. Please check it later, or contact support team if upgrade is required urgently.";
              return {result:"info", message:message};
            }
          }

          var resp = api.system.service.GetVersion();
          if (resp.result != 0) return {result:"error", message:"Cannot determine platform version!"};

          if (compareVersions([resp.version, resp.build].join("."), '6.1.4') >= 0) {
            resp = api.env.control.GetNodeGroups("${env.name}", session);
            if (resp.result !=0) return {result:"error", message:"Cannot obtain node groups!"};
            var groups = resp.object, next = "${settings.uplist}", upgrades = [], version;
            for (var i = 0, n = groups.length; i < n; i++)
            if (groups[i].name == "k8sm" && groups[i].version && groups[i].avail) {
              upgrades = groups[i].avail.split(",");
              version = groups[i].version;
              break;
            }
            if (!upgrades.length || !version) return {result:"warning", message:"Cannot determine cluster upgrade parameters!"};

            while (upgrades.length && compareVersions(next, upgrades[0]) >= 0) upgrades.shift();

            return startUpgrade(version, next, upgrades);

          } else {
            var envName = "${env.envName}", nodeId = "${nodes.k8sm.master.id}";
            resp = jelastic.env.control.GetNodeInfo(envName, session, nodeId);
            if (resp.result != 0) return {result:"error", message:"Cannot obtain node info!"};
            var version = resp.node.version;
            resp = jelastic.env.control.GetContainerNodeTags(envName, session, nodeId);
            if (resp.result != 0) return {result:"error", message:"Cannot obtain node tags!"};

            var tags = resp.object;
            tags.sort(compareVersions);
            var check_version = version, upgrades = [];
            var major_version = version.substr(0, version.lastIndexOf("."));

            for (var i = 0; i < tags.length; i++) {
              var major_tag = tags[i].substr(0, tags[i].lastIndexOf("."));
              if (compareVersions(major_tag, major_version) > 0) {
                check_version = tags[i];
                upgrades.push(check_version);
                major_version = major_tag;
              }
            }

            var last_version = tags.pop();
            if (compareVersions(last_version, check_version) > 0) upgrades.push(last_version);
            if (!upgrades.length) {
              var message = "Current version " + version + " is the latest. No upgrades are available.";
              return {result:"info", message:message};
            }

            upgrades.sort(compareVersions);
            var next = upgrades.shift();

            return startUpgrade(version, next, upgrades);
          }

      addon-upgrade-start:
        jps: ${this.jps}
        envName: ${env.envName}
        current: ${this.current}
        version: ${this.next}
        avail: ${this.avail}
        upgradeScript: |
          jelastic.marketplace.jps.Install({ envName: envName, session: session, jps: jps, settings: { version: version, avail: avail } });
          return { result: 0 };
        script: |
          var envName = '${env.envName}',
          scriptName = envName + '-k8s-upgrade';
          jelastic.dev.scripting.DeleteScript(scriptName);
          resp = jelastic.dev.scripting.CreateScript(scriptName, "js", upgradeScript);
          if (resp.result != 0) return resp;
          java.lang.Thread.sleep(1000);
          jelastic.dev.scripting.Build(scriptName);
          resp = jelastic.utils.scheduler.AddTask({ script: scriptName, trigger: "once_delay:1000", description: "Upgrade Kubernetes", params: { envName: envName, jps: jps, version: version, avail: avail } });
          if (resp.result != 0) return resp;
          java.lang.Thread.sleep(3000);
          return { type: "info", message: "Kubernetes Cluster " + current + " upgrade to " + version + " has been started.\n\nThe update process may take several minutes depending on number of nodes and deployed services." };

  - id: certman-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.9
    name: Certificate Manager
    description: |
      Kubernetes SSL Certificate Manager allows to bind custom domain names
      to the cluster and manage SSL certificates
    logo: /images/k8s-cert.png
    settings:
      fields:
        - type: displayfield
          hideLabel: true
          markup: Please specify an external DNS name to assign the certificate with.
        - type: displayfield
          hideLabel: true
        - type: string
          caption: External domain
          name: certificate_domain
      onBeforeInit: /scripts/certAddonOnBeforeInit.js  
    buttons:
      - caption: Install Certificate Manager
        settings: main
        action: addon-certman-config
        loadingText: Installing...
        successText: Certficate manager has been successfully installed!

    actions:
      addon-certman-config:
      - cmd[${nodes.k8sm.master.id}]: kubectl -n cert-manager get deployment cert-manager &>/dev/null && echo "true" || echo "false"
      - if ('${response.out}' == 'false'):
          - install-cert-manager
      - cmd[${nodes.k8sm.master.id}]: |-
          nginx_chart_version=4.7.2
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm install cert-manager-nginx ingress-nginx/ingress-nginx --namespace cert-manager --version ${nginx_chart_version} --set controller.ingressClassResource.name=nginx-cert --set controller.ingressClassResource.controllerValue="k8s.io/ingress-nginx-cert" --set controller.ingressClassResource.enabled=true --set controller.ingressClass=nginx-cert --set controller.ingressClassByName=true
          wait-deployment.sh cert-manager-nginx-ingress-nginx-controller cert-manager 1 720
          kubectl apply -f ${baseUrl}/addons/cert-manager/helloworld-service.yaml
          wget "${baseUrl}/addons/cert-manager/helloworld-ingress.yaml" -O /tmp/helloworld-ingress.yaml
          sed -i 's/example\.com/${settings.certificate_domain}/g' /tmp/helloworld-ingress.yaml
          kubectl apply -f /tmp/helloworld-ingress.yaml
          kubectl create secret generic -n cert-manager jelastic-domain --from-literal=certificate_domain=${settings.certificate_domain}

  - id: rancher-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.9
    name: Rancher Installer
    description: Rancher Management Platform
    logo: /images/k8s-rancher.png

    buttons:
      - caption: Rancher Platform Installation
        action: addon-rancher
        loadingText: Installing...
        confirmText: Do you want to install Rancher Management Platform?

    actions:
      addon-rancher:
      - cmd[${nodes.k8sm.master.id}]: kubectl -n cattle-system get deployment rancher &>/dev/null && echo "true" || echo "false"
      - if ('${response.out}' == 'true'):
          return:
            type: info
            message: Rancher Management Platform is already installed!
      - init-globals-ingress
      - if ('${globals.ingress-dir}' != 'nginx'):
          return:
            type: warning
            message: Rancher Platforms requires 'nginx' ingress controller! The current ingress controller is '${globals.ingress-dir}'
      - cmd[${nodes.k8sm.master.id}]: |-
          kubectl delete -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/helloworld-ingress.yaml ||:
          kubectl delete -f ${baseUrl}/addons/helloworld.yaml ||:
      - cmd[${nodes.k8sm.master.id}]: |-
          http_code=$(curl -Lks -o /dev/null -w "%{http_code}" "${env.url}")
          ret_code=$?
          if [ ${ret_code} -ne 0 ]; then
            echo "An error occurred while accessing the [endpoint](${env.url})"
          elif [ ${http_code} -eq 200 ]; then
            echo "The [endpoint](${env.url}) is already taken"
          elif [ ${http_code} -ne 404 ]; then
            echo "The [endpoint](${env.url}) is present and answered with HTTP code ${http_code}"
          fi
      - if ('${response.out}'):
          return:
            type: warning
            message: Cannot deploy Rancher UI! ${response.out}.
      - cmd[${nodes.k8sm.master.id}]: cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1
      - set:
          rancher_secret: ${response.out}
      - cmd[${nodes.k8sm.master.id}]: |-
          helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
          helm repo update
          helm install rancher rancher-stable/rancher --create-namespace --namespace cattle-system --set tls=external --set bootstrapPassword="${this.rancher_secret}"
          kubectl -n cattle-system rollout status deploy/rancher
      - message.email.send:
          to: "${user.email}"
          subject: Rancher Platform Successfully Installed in ${env.name}
          body: |-
            Rancher Management Platform installed in <b>${env.name}</b> Kubernetes Cluster: <br>
            Rancher Dashboard - ${env.url}<br>
            Your login password: ${this.rancher_secret}
      - return:
          type: success
          message: |
            Rancher Platform has been successfully installed!

            Enter [Rancher dashboard](${env.url}), using password:

            ```${this.rancher_secret}```

  - id: regcreds-k8s-addon
    type: update
    permanent: true
    baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.9
    name: DockerHub Registry Credentials
    description: |
      Leverage DockerHub images pull rate limits: assign DockerHub user credentials to Kubernetes deployments cluster-wide
    logo: /images/k8s-regcreds.png
    settings:
      fields:
        - type: displayfield
          hideLabel: true
          markup: Please specify DockerHub user's valid credentials in the fields below. All previously stored DockerHub user credentials (if any) will be overwritten!
        - type: displayfield
          hideLabel: true
        - type: string
          required: true
          regex: "^((?!\\s).)*$"
          regexText: Incorrect username
          caption: Username
          name: creds_username
        - type: string
          required: true
          inputType: password
          caption: Password
          name: creds_password
        - type: string
          required: true
          vtype: email
          caption: E-mail
          name: creds_email
    buttons:
      - caption: DockerHub Credentials
        settings: main
        action: addon-regcreds-config
        loadingText: Setting up...
        successText: DockerHub registry user credentials have been successfully set!

    actions:
      addon-regcreds-config:
      - cmd[${nodes.k8sm.master.id}]: |-
          kubectl -n registry-creds-system delete pods -l control-plane=registry-creds-controller
          kubectl apply -f ${baseUrl}/addons/registry-creds/registry-creds-system.yaml
          wait-deployment.sh registry-creds-registry-creds-controller registry-creds-system 1 600
          kubectl delete -f ${baseUrl}/addons/registry-creds/dockerhub-secret.yaml
          kubectl -n kube-system delete secret dockerhub-credentials-secret
          kubectl -n kube-system create secret docker-registry dockerhub-credentials-secret --docker-username='${settings.creds_username}' --docker-password='${settings.creds_password}' --docker-email='${settings.creds_email}'
          kubectl apply -f ${baseUrl}/addons/registry-creds/dockerhub-secret.yaml

success: |
  ${globals.default_success:}
  ${globals.monitoring_success:}
  ${globals.jaeger_success:}
  ${globals.check_message:}

  Press **Open in Browser** to view a default web page of your application.
