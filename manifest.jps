type: install
version: 1.5
id: kubernetes
baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.15.5
description:
  text: /text/description-kube.md
  short: Kubernetes Cluster
categories:
  - apps/clusters
  - apps/dev-and-admin-tools

logo: /images/k8s-logo.png
name: Kubernetes Cluster
targetRegions:
  type: vz7

ssl: true
onBeforeInit: /scripts/beforeinit.js
onBeforeInstall: /scripts/beforeinstall.js

nodes: definedInOnBeforeInstall

skipNodeEmails: true

globals:
    dashboardUrl:  https://${env.domain}/kubernetes-dashboard/

onInstall:
  - block-masters-scaling
  - init-main-master
  - forEach(node:nodes.k8sm):
      if (!${@node.ismaster}):
        init-slave-master:
          id: ${@node.id}
          ip: ${@node.intIP}

  - connect-workers: cp
  - setup-overlay
  - if ('${settings.ingress-controller}' == 'Nginx'):
      - setGlobals:
          ingress-dir: nginx
      - install-nginx
  - elif ('${settings.ingress-controller}' == 'HAProxy'):
      - setGlobals:
          ingress-dir: haproxy
      - install-haproxy
  - else:
      - setGlobals:
          ingress-dir: Traefik
      - install-traefik
  - configure-remote-api
  - install-components
  - install-helm-master
  - forEach(node:nodes.k8sm):
      if (!${@node.ismaster}):
        install-helm-slave:
          id: ${@node.id}
  - install-node-problem-detector
  - generate-admin-token
  - connect-storage
  - deploy
  - remove-attr

  - if (${settings.api:true}):
      - setup-remote-api: true

  - if (${settings.monitoring:true}):
        - install-monitoring

  - if ('${env.protocol}' == 'http'):
      - api: env.control.AddEndpoint
        nodeId: ${nodes.cp.master.id}
        privatePort: 30777
        protocol: TCP
        name: Dashboard Self-Signed HTTPS
      - setGlobals:
          dashboardUrl: https://node${nodes.cp.master.id}-${env.domain}:${response.object.publicPort}/

  - setGlobals:
        default_success: |
             Enter [Kubernetes dashboard](${globals.dashboardUrl}) ${globals.default_api:} using the Access Token:

             ```${globals.token}```
             Press **Open in Browser** to view a default web page of your application.
             To bind a custom domain name with your Kubernetes cluster please refer to the steps described in Jelastic [documentation](https://docs.jelastic.com/custom-domains).
  - check-health

onAfterScaleOut[cp]:
  - get-ids: event.response.nodes
  - connect-workers: ${globals.ids}

onBeforeScaleIn[cp]:
  forEach(event.response.nodes):
    removeWorker:
      workerHostname: node${@i.id}-${env.domain}

onBeforeClone: stopEvent

onBeforeRedeployContainer:
  if (!${event.params.skipReinstall:false}): stopEvent

actions:
  block-masters-scaling:
    env.control.ApplyNodeGroupData[k8sm]:
      data:
        validation:
          minCount: ${nodes.k8sm.length}
          maxCount: ${nodes.k8sm.length}

  init-main-master:
    - if (${nodes.mbl.length:0}):
        cmd[mbl]: |-
          sed -i '/^<\/mappings>.*/i \\t<pair frontend_port="6443" backend_port="6443" description="CPlane balancing" option="tcp-check" params="check fall 3 rise 2">' /etc/haproxy/tcpmaps/mappings.xml
          sed -i 's/^bind :::80/#bind :::80/g' /etc/haproxy/haproxy.cfg
          sed -i '/^daemon$/a stats socket /var/run/haproxy.sock mode 660 level admin' /etc/haproxy/haproxy.cfg
          sed -i '/^daemon$/a stats timeout 2m' /etc/haproxy/haproxy.cfg
          echo '${nodes.k8sm.master.intIP}' > /etc/haproxy/hosts
          jem balancer rebuildCommon
        user: root
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        entryPoint=$((( ${nodes.mbl.length:0} > 0 )) && echo mbl || echo k8sm)
        sed -i "s/^controlPlaneEndpoint:.*/controlPlaneEndpoint: \"${entryPoint}.${env.domain}:6443\"/g" /etc/kubernetes/custom-kubeadm.yaml
        kubeadm init --config /etc/kubernetes/custom-kubeadm.yaml --upload-certs --ignore-preflight-errors=swap,numcpu | tee /var/log/kubeadm-init.log
        sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep 'control-plane' > /var/lib/connect/settings-master
        sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep -v 'control-plane' > /var/lib/connect/settings
        printf "0 1 * * * kubeadm token create --print-join-command  > /var/lib/connect/settings\n\n\n" > /var/spool/cron/root
    - configure-master: ${nodes.k8sm.master.id}
    - if (${settings.api:true}):
        - setGlobals:
            default_api: or [Remote API Endpoint](${env.protocol}://${env.domain}/api/)

  init-slave-master:
    - cmd[${this.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        $(cat /var/lib/connect/settings-master) --ignore-preflight-errors=swap,numcpu > /dev/null 2>&1
    - configure-master: ${this.id}
    - cmd[mbl]: |-
        echo '${this.ip}' >> /etc/haproxy/hosts
        jem balancer rebuildCommon
      user: root

  configure-master:
    cmd[${this}]: |-
      mkdir -p $HOME/.kube
      cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      chown root:root $HOME/.kube/config
      wget -qO- https://github.com/derailed/k9s/releases/download/0.8.4/k9s_0.8.4_Linux_x86_64.tar.gz | tar xz -C /usr/bin k9s
      wget -qO- https://github.com/derailed/popeye/releases/download/v0.4.3/popeye_0.4.3_Linux_x86_64.tar.gz | tar xz -C /usr/bin popeye
      wget https://github.com/wercker/stern/releases/download/1.11.0/stern_linux_amd64 -O /usr/bin/stern
      chmod +x /usr/bin/stern
      /usr/bin/stern --completion=bash > /etc/bash_completion.d/stern.bash
      kubectx_version=0.7.0
      wget -qO- https://github.com/ahmetb/kubectx/archive/v${kubectx_version}.tar.gz | tar xz --strip-components=1 -C /usr/bin kubectx-${kubectx_version}/kubectx kubectx-${kubectx_version}/kubens
      wget -qO- https://github.com/ahmetb/kubectx/archive/v${kubectx_version}.tar.gz | tar xz --strip-components=2 -C /etc/bash_completion.d kubectx-${kubectx_version}/completion/kubens.bash kubectx-${kubectx_version}/completion/kubectx.bash
      wget https://github.com/weaveworks/weave/releases/download/v2.5.2/weave -O /usr/bin/weave
      chmod +x /usr/bin/weave
      iptables -I INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
      service iptables save
      systemctl start kube-postconf.service
      systemctl enable docker.service
      systemctl enable kubelet.service
      systemctl enable kube-postconf.service

  connect-workers:
    cmd[${this}]: |-
      rm -f /etc/machine-id
      systemd-machine-id-setup
      systemctl daemon-reload > /dev/null 2>&1
      $(cat /var/lib/connect/settings) --ignore-preflight-errors=swap,numcpu > /dev/null 2>&1
      sleep 5
      iptables -I INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
      service iptables save
      systemctl start kube-postconf.service
      systemctl enable docker.service
      systemctl enable kubelet.service
      systemctl enable kube-postconf.service

  setup-overlay:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/weave-pack.yaml

  install-nginx:
     cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/nginx/mandatory.yaml
      kubectl apply -f ${baseUrl}/addons/nginx/cloud-generic.yaml

  install-haproxy:
     cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/haproxy/haproxy-deployment.yaml

  install-traefik:
     cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-rbac.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-ds.yaml
      kubectl apply -f ${baseUrl}/addons/traefik/traefik-ui.yaml

  configure-remote-api:
      - if (${settings.api:true}):
          - cmd[${nodes.k8sm.master.id}]: |-
              kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/api-ingress.yaml
          - setGlobals:
              default_api: or [Remote API Endpoint](${env.protocol}://${env.domain}/api/)

  install-components:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl create -f ${baseUrl}/addons/metrics-server/aggregated-metrics-reader.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/auth-delegator.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/auth-reader.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-apiservice.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-server-deployment.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-server-service.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/resource-reader.yaml
        kubectl create -f ${baseUrl}/addons/create-admin.yaml
        kubectl create -f ${baseUrl}/addons/grant-privileges.yaml

    - if ('${settings.dashboard}' == 'version1'):
        cmd[${nodes.k8sm.master.id}]: |-
           kubectl create -f ${baseUrl}/addons/kubernetes-dashboard.yaml
           kubectl create -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/dashboard-ingress.yaml

    - if ('${settings.dashboard}' == 'version2'):
        cmd[${nodes.k8sm.master.id}]: |-
           kubectl create -f ${baseUrl}/addons/kubernetes-dashboard-beta.yaml
           kubectl create -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/dashboard-ingress-beta.yaml

  install-helm-master:
    cmd[${nodes.k8sm.master.id}]: |-
      curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash > /dev/null 2>&1
      helm init --upgrade
      helm repo update
      kubectl create serviceaccount --namespace kube-system tiller
      kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
      kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
      while true; do kubectl get pods --field-selector=status.phase=Running -n kube-system | grep tiller && break ; done
      sleep 5

  install-helm-slave:
    cmd[${this.id}]: |-
      curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash > /dev/null 2>&1
      helm init --client-only
      helm repo update

  generate-admin-token:
    - cmd[${nodes.k8sm.master.id}]: kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep fulladmin | awk '{print $1}')  | grep 'token:' | sed -e's/token:\| //g'
    - setGlobals:
        token: ${response.out}

  deploy:
    - if ('${settings.deploy}' == 'cc'):
        cmd[${nodes.k8sm.master.id}]: |-
          kubectl apply -f ${baseUrl}/addons/helloworld.yaml
          kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/helloworld-ingress.yaml
    - if ('${settings.deploy}' == 'cmd'):
        cmd[${nodes.k8sm.master.id}]: |-
          ${settings.cmd}
          if [ '${settings.ingress-controller}' == "Nginx" ]; then kubectl delete ingress/open-liberty-traefik ingress/open-liberty-haproxy || true; fi
          if [ '${settings.ingress-controller}' == "HAProxy" ]; then kubectl delete ingress/open-liberty-traefik ingress/open-liberty-nginx || true; fi
          if [ '${settings.ingress-controller}' == "Traefik" ]; then kubectl delete ingress/open-liberty-haproxy ingress/open-liberty-nginx || true; fi
    - if ('${settings.deploy}' == 'yml'):
        cmd[${nodes.k8sm.master.id}]: kubectl apply -f ${settings.yml}

  connect-storage:
    if (${settings.storage:false}):
      cmd[${nodes.k8sm.master.id}]: helm install stable/nfs-client-provisioner --set nfs.server=${nodes.storage.master.address} --set nfs.path=/data --set nfs.mountOptions='{soft,proto=tcp}' --set replicaCount=3 --set storageClass.defaultClass=true --set storageClass.allowVolumeExpansion=true --set storageClass.name=jelastic-dynamic-volume

  removeWorker:
    cmd[${nodes.k8sm.master.id}]: |-
      /usr/bin/kubectl drain ${this.workerHostname} --ignore-daemonsets --delete-local-data || exit 8;
      /usr/bin/kubectl delete node ${this.workerHostname} || exit 9;

  remove-attr:
    cmd[*]: |-
      chattr -i -a /root/.bash_*
    user: root

  setup-remote-api:
  - log: '${this}'
  - cmd[${nodes.k8sm.master.id}]: |-
      action=$([ "${this}" == "true" ] && echo "apply" || echo "delete")
      kubectl $action -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/api-ingress.yaml

  install-monitoring:
  - log: '${globals.monitoring_installed}'
  - if ('${globals.monitoring_installed}' == 'true'):
        return:
            type: info
            message: ${globals.monitoring_success}

  - cmd[${nodes.k8sm.master.id}]: |-
      [ ! -f /root/monitoring_is_installed ] && {
      helm repo update
      helm install --name monitoring-prometheus --namespace kubernetes-monitoring stable/prometheus --set server.prefixURL=/prometheus --set server.baseURL=/prometheus
      while true; do kubectl get pods --field-selector=status.phase=Running -n kubernetes-monitoring | grep prometheus-server && break ; done
      helm fetch stable/grafana --untar
      for dash_name in "kubernetes-prometeus-dashboard" "kubernetes-rchakra3-dashboard" "kubernetes-vanniekerk-dashboard"; do
        wget "${baseUrl}/addons/monitoring/${dash_name}.json" -O "grafana/dashboards/${dash_name}.json"
      done
      sleep 5
      helm install --name monitoring-grafana --namespace kubernetes-monitoring --set 'grafana\.ini'.server.root_url=${env.url}grafana -f ${baseUrl}/addons/monitoring/jelastic-values.yaml grafana/.
      while true; do kubectl get pods --field-selector=status.phase=Running -n kubernetes-monitoring | grep grafana && break ; done
      sleep 5
      grafana_secret=$(kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}' | base64 --decode ; echo)
      if [ "${settings.ingress-controller}" == "HAProxy" ]; then export crypt="-d"; fi
      kubectl create secret generic monitoring-prometheus --from-literal=auth="$(htpasswd ${crypt} -nb admin ${grafana_secret})" --namespace=kubernetes-monitoring
      kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/prometheus-ingress.yaml
      kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/alert-ingress.yaml
      kubectl create -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/grafana-ingress.yaml
      sleep 20
      curl -X POST -d "user=admin&password=${grafana_secret}" -c grafana/grafana-jar.txt "http://${env.domain}/grafana/login"
      dash_id=$(curl -sb grafana/grafana-jar.txt 'http://${env.domain}/grafana/api/search?mode=tree&query=Jelastic' | grep -Po '"id":(\d+)' | awk -F ':' '{print $2}')
      curl -X POST -b grafana/grafana-jar.txt "http://${env.domain}/grafana/api/user/stars/dashboard/${dash_id}"
      curl -X PUT -H 'Content-Type: application/json' -b grafana/grafana-jar.txt -d "{\"homeDashboardId\":${dash_id}}" "http://${env.domain}/grafana/api/org/preferences"
      touch /root/monitoring_is_installed; } || echo "Already installed" ;
  - cmd[${nodes.k8sm.master.id}]: kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}' | base64 --decode
  - setGlobals:
      grafana_secret: ${response.out}
  - setGlobals:
      monitoring_installed: true
  - log: ${globals.monitoring_installed}

  - setGlobals:
      monitoring_success: |
          Enter [Prometheus dashboard](${env.url}prometheus/), [Prometheus AlertManager](${env.url}prometheus-alert/)
          and [Grafana dashboard](${env.url}grafana/), using login "admin" and password:

          ```${globals.grafana_secret}```
  - if (!${settings.monitoring:true}):
      jelastic.message.email.SendToUser:
            login: "${user.email}"
            subject: Monitoring Tools Successfully Installed in ${env.name}
            body: |-
              Monitoring Tools installed in <b>${env.name}</b> Kubernetes Cluster: <br>
              Prometheus Dashboard - ${env.url}prometheus/<br>
              Prometheus AlertManager - ${env.url}prometheus-alert/ <br>
              Grafana Dashboard - ${env.url}grafana/ <br>
              Credentials - admin / ${globals.grafana_secret}
      return:
        type: info
        message: ${globals.monitoring_success}

  get-ids:
    - setGlobals:
        ids: ''
        sep: ''
    - forEach(${this}):
        add-id: ${@i.id}

  add-id:
    setGlobals:
      ids: ${globals.ids:}${globals.sep:}${this}
      sep: ','

  install-node-problem-detector:
     - cmd[${nodes.k8sm.master.id}]: helm install stable/node-problem-detector --name node-problem-detector --set image.tag=v0.7.0

  check-health:
      cmd[${nodes.k8sm.master.id}]: curl -s ${baseUrl}/scripts/check-install.sh | bash -s -- -i=${settings.ingress-controller} -app=${settings.deploy} -m=${settings.monitoring} -r=${settings.api} -s=${settings.storage} -d=${env.domain}  &> /var/log/k8s-health-check.log || echo "*Warning! Some cluster components have not yet been initialized, and it may take some time for pods to start. If you encounter problems with your cluster, please check K8s logs in /var/log on master node and contact support.*"
      setGlobals:
         check_message: ${response.out}

addons:

  - id: conf-k8s-addon
    type: update
    baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.15.5
    name: Kubernetes Cluster Configuration
    description: Configure remote API access and monitoring tools or upgrade your Kubernetes cluster
    logo: /images/k8s-logo.png
    settings:
      fields:
        - type: displayfield
          caption: Useful info
          hideLabel: true
          markup: Access and manage the cluster remotely via API
        - type: checkbox
          name: api
          caption: Remote API access is enabled
        - type: string
          name: monitoring
          inputType: hidden
    buttons:
      - caption: Remote API
        settings: main
        action: addon-remote-api
        loadingText: Updating...
        confirmText: Are you sure?
        successText: Remote API Access was successfully updated!
      - caption: Upgrade
        action: addon-upgrade
        loadingText: Updating...
        confirmText: Do you want to upgrade Kubernetes Cluster?
        successText: Kubernetes Cluster has been successfully upgraded!
      - caption: Monitoring
        action: install-monitoring
        confirmText: Monitoring tools will be installed if missing. Continue?

    actions:
      addon-upgrade:
        script: |
          var envName = "${env.envName}", nodeId = "${nodes.k8sm.master.id}";
          var resp = jelastic.env.control.GetNodeInfo(envName, session, nodeId);
          if (resp.result != 0) return resp;
          var version = resp.node.version;
          var image = resp.node.name;
          resp = jelastic.env.control.GetContainerNodeTags(envName, session, nodeId);
          if (resp.result != 0) return resp;
          var tags = resp.object;
          var upgrades = [];
          for (var i = 0; i < tags.length; i++) if (tags[i] > version) upgrades.push(tags[i]);
          var message = "Current version " + version + " is the latest. No upgrades are available.";
          if (upgrades.length) {
            upgrades.sort();
            var next = upgrades.pop();
            var baseUrl = "${baseUrl}".split("/"); baseUrl.pop(); baseUrl = baseUrl.join("/");
            var url = baseUrl+"/"+next+"/addons/upgrade.jps"
            var huc = new java.net.URL(url).openConnection();
            huc.setRequestMethod("HEAD");
            var code = huc.getResponseCode();
            if (code == 200){
              return {result:0, onAfterReturn:{execUpgrade:{current:version, next:next, avail:upgrades.join(", "), jps: url}}};
            } else {
              message = "The next version is " + next + ". However, automated upgrade procedure is not available yet. Please check it later, or contact support team if upgrade is required urgently.";
              return {result:"info", message:message};
            }
          } else {
            return {result:"info", message:message};
          }

      execUpgrade:
        install: ${this.jps}
        envName: ${env.envName}
        settings:
            version: ${this.next}

      addon-remote-api:
      - log: '${this.api}'
      - setup-remote-api: ${this.api}
      - if (${this.api:true}):
          - setGlobals:
              apiStatusMessage: enabled at ${env.url}api
      - else:
          - setGlobals:
              apiStatusMessage: disabled
      - jelastic.message.email.SendToUser:
          login: "${user.email}"
          subject: Remote API Access Successfully Updated in ${env.name}
          body: Remote API has been ${globals.apiStatusMessage}

success: |
  ${globals.default_success:}
  ${globals.monitoring_success:}
  ${globals.check_message:}
