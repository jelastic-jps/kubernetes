type: install
version: 1.5
id: kubernetes
baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/master
description:
  text: /text/description-kube.md
  short: Kubernetes Cluster
categories:
  - apps/clusters
  - apps/dev-and-admin-tools

logo: /images/k8s-logo.png
name: Kubernetes Cluster
targetRegions:
  type: vz7

settings:
  fields:
    - name: deploy
      type: radio-fieldset
      values:
        cc: Clean cluster with pre-deployed HelloWorld example
        cmd: Deploy custom helm or stack via shell command
      default: cc
      showIf:
        cmd:
          name: cmd
          type: text
          height: 65
          required: true
          hideLabel: true
          default: |-
            helm repo add ibm-charts https://raw.githubusercontent.com/IBM/charts/master/repo/stable/
            helm install --name default --set autoscaling.enabled=true --set autoscaling.minReplicas=2 ibm-charts/ibm-open-liberty --version 1.6.0  --debug
            kubectl apply -f https://raw.githubusercontent.com/jelastic-jps/kubernetes/master/addons/openliberty.yaml

    - name: label
      caption: Topology
      type: displayfield
    - name: topo
      type: radio-fieldset
      values:
        0-dev: '<b>Development:</b> one master (1) and one scalable worker (1+)'
        1-prod: '<b>Production:</b> multi master (3) with API balancers (2+) and scalable workers (2+)'
      default: 0-dev

    - type: checkbox
      name: storage
      caption: Attach dedicated NFS Storage with dynamic volume provisioning
      value: true

    - type: checkbox
      name: api
      caption: Enable Remote API Access
      value: false

ssl: true

onBeforeInstall: |
  var url = "https://registry.hub.docker.com/v1/repositories/jelastic/kubernetes/tags",
    tags = toNative(new com.hivext.api.core.utils.Transport().get(url)).sort(),
    latest = "latest";
  for (var i = 0; i < tags.length; i++) if (tags[i].name > latest) latest = tags[i].name;
  var k8smCount = '${settings.topo}' == '0-dev' ? 1 : 3,
      workerCount = k8smCount > 1 ? 2 : 1;
  var resp = {
    result: 0,
    ssl: !!jelastic.billing.account.GetQuotas('environment.jelasticssl.enabled').array[0].value,
    nodes: [{
      count: k8smCount,
      cloudlets: 32,
      nodeType: "kubernetes",
      nodeGroup: "k8sm",
      displayName: "Master",
      extip: false,
      env: {
        JELASTIC_EXPOSE: false
      },
      volumes: [
        "/var/lib/connect"
      ],
      volumeMounts: {
        "/var/lib/connect": {
          readOnly: true,
          sourcePath: "/var/lib/connect",
          sourceNodeGroup: "k8sm"
        }
      }
    }, {
      count: workerCount,
      nodeGroup: "cp",
      nodeType: "kubernetes",
      displayName: "Workers",
      cloudlets: 32,
      extip: false,
      env: {
        JELASTIC_EXPOSE: false
      },
      volumes: [
        "/var/lib/connect"
      ],
      volumeMounts: {
        "/var/lib/connect": {
          readOnly: true,
          sourcePath: "/var/lib/connect",
          sourceNodeGroup: "k8sm"
        }
      }
    }]
  }

  if (k8smCount > 1) {
    resp.nodes.push({
      count: 2,
      nodeType: "haproxy",
      cloudlets: 8,
      displayName: "API Balancer",
      nodeGroup: "mbl",
      env: {
        JELASTIC_PORTS: 6443
      }
    })
  }

  if (${settings.storage:false}) {
    var path = "/data";
    resp.nodes.push({
      count: 1,
      image: "jelastic/storage",
      cloudlets: 8,
      displayName: "Storage",
      nodeGroup: "storage",
      volumes: [
        path
      ]
    })

    for (var i = 0; i < 2; i++){
      var n = resp.nodes[i];
      n.volumes.push(path);
      n.volumeMounts[path] = {
          readOnly: false,
          sourcePath: path,
          sourceNodeGroup: "storage"
      };
    }
  }
  return resp;

nodes: definedInOnBeforeInstall

skipNodeEmails: true

onInstall:
  - block-masters-scaling
  - init-main-master
  - forEach(node:nodes.k8sm):
      if (!${@node.ismaster}):
        init-slave-master:
          id: ${@node.id}
          ip: ${@node.intIP}

  - connect-workers: cp
  - setup-overlay
  - install-components
  - install-helm
  - install-traefik
  - manage-ingress-routes
  - generate-admin-token
  - connect-storage
  - deploy

onAfterScaleOut[cp]:
  forEach(event.response.nodes):
    connect-workers: ${@i.id}

onBeforeScaleIn[cp]:
  forEach(event.response.nodes):
    removeWorker:
      workerHostname: node${@i.id}-${env.domain}

actions:
  block-masters-scaling:
    env.control.ApplyNodeGroupData[k8sm]:
      data:
        validation:
          minCount: ${nodes.k8sm.length}
          maxCount: ${nodes.k8sm.length}

  init-main-master:
    - if (${nodes.mbl.length:0}):
        cmd[mbl]: |-
          sed -i '/^<\/mappings>.*/i \\t<pair frontend_port="6443" backend_port="6443" description="CPlane balancing" option="tcp-check" params="check fall 3 rise 2">' /etc/haproxy/tcpmaps/mappings.xml
          sed -i 's/^bind :::80/#bind :::80/g' /etc/haproxy/haproxy.cfg
          echo '${nodes.k8sm.master.intIP}' > /etc/haproxy/hosts
          jem balancer rebuildCommon
        user: root
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        entryPoint=$((( ${nodes.mbl.length:0} > 0 )) && echo mbl || echo k8sm)
        sed -i "s/^controlPlaneEndpoint:.*/controlPlaneEndpoint: \"${entryPoint}.${env.domain}:6443\"/g" /etc/kubernetes/custom-kubeadm.yaml
        kubeadm init --config /etc/kubernetes/custom-kubeadm.yaml --experimental-upload-certs --ignore-preflight-errors=swap | tee /var/log/kubeadm-init.log
        sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep 'experimental-control-plane' > /var/lib/connect/settings-master
        sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep -v 'experimental-control-plane' > /var/lib/connect/settings
    - configure-master: ${nodes.k8sm.master.id}
    - if (${settings.api:true}):
        cmd[${nodes.k8sm.master.id}]: |-
           kubectl apply -f ${baseUrl}/addons/api-ingress.yaml

  init-slave-master:
    - cmd[${this.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        $(cat /var/lib/connect/settings-master) --ignore-preflight-errors=swap > /dev/null 2>&1
    - configure-master: ${this.id}
    - cmd[mbl]: |-
        echo '${this.ip}' >> /etc/haproxy/hosts
        jem balancer rebuildCommon
      user: root

  configure-master:
    cmd[${this}]: |-
      mkdir -p $HOME/.kube
      cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      chown root:root $HOME/.kube/config
      iptables -I INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
      service iptables save
      systemctl enable kubelet.service

  manage-ingress-routes:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/dashboard-ingress.yaml

  connect-workers:
    cmd[${this}]: |-
      systemctl daemon-reload > /dev/null 2>&1
      $(cat /var/lib/connect/settings) --ignore-preflight-errors=swap > /dev/null 2>&1
      sleep 5
      iptables -I INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
      service iptables save
      systemctl enable kubelet.service

  setup-overlay:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/weave-pack.yaml
      wget https://github.com/weaveworks/weave/releases/download/v2.5.2/weave -O /usr/bin/weave
      chmod +x /usr/bin/weave

  install-components:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl create -f ${baseUrl}/addons/metrics-server/aggregated-metrics-reader.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/auth-delegator.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/auth-reader.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-apiservice.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-server-deployment.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-server-service.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/resource-reader.yaml
        kubectl create -f ${baseUrl}/addons/kubernetes-dashboard.yaml
        kubectl create -f ${baseUrl}/addons/create-admin.yaml
        kubectl create -f ${baseUrl}/addons/grant-privileges.yaml

  install-helm:
    cmd[${nodes.k8sm.master.id}]: |-
      curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash > /dev/null 2>&1
      helm init --upgrade
      helm repo update
      kubectl create serviceaccount --namespace kube-system tiller
      kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
      kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
      while true; do kubectl get pods --field-selector=status.phase=Running -n kube-system | grep tiller && break ; done
      sleep 5

  install-traefik:
     cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/traefik-rbac.yaml
      kubectl apply -f ${baseUrl}/addons/traefik-ds.yaml
      kubectl apply -f ${baseUrl}/addons/traefik-ui.yaml

  generate-admin-token:
    - cmd[${nodes.k8sm.master.id}]: kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep fulladmin | awk '{print $1}')  | grep 'token:' | sed -e's/token:\| //g'
    - setGlobals:
        token: ${response.out}

  deploy:
    - if ('${settings.deploy}' == 'cc'):
        cmd[${nodes.k8sm.master.id}]: |-
          kubectl apply -f ${baseUrl}/addons/helloworld.yaml
    - if ('${settings.deploy}' == 'cmd'):
        cmd[${nodes.k8sm.master.id}]: ${settings.cmd}
    - if ('${settings.deploy}' == 'yml'):
        cmd[${nodes.k8sm.master.id}]: kubectl apply -f ${settings.yml}

  connect-storage:
    if (${settings.storage:false}):
      cmd[${nodes.k8sm.master.id}]: helm install stable/nfs-client-provisioner --set nfs.server=${nodes.storage.master.address} --set nfs.path=/data --set replicaCount=3 --set storageClass.defaultClass=true --set storageClass.allowVolumeExpansion=true --set storageClass.name=jelastic-dynamic-volume


  removeWorker:
    cmd[${nodes.k8sm.master.id}]: |-
      /usr/bin/kubectl drain ${this.workerHostname} --ignore-daemonsets --delete-local-data || exit 8;
      /usr/bin/kubectl delete node ${this.workerHostname} || exit 9;


success: /text/success.md
