version: 1.5
type: update
id: kubernetes-upgrade-to-1.17.4
name: Kubernetes Upgrade to 1.17.4

categories:
  - apps/dev-and-admin-tools

homepage: https://github.com/jelastic-jps/kubernetes
baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.17.4
logo: /images/k8s-logo.png

description:
  text: "K8s upgrade"
  short: Kubernetes Upgrade

onInstall:
  - upgrade-configuration
  - upgrade-masters-cluster:
      id: ${nodes.k8sm.master.id}
      master: true
      hostname: node${nodes.k8sm.master.id}-${env.domain}
      version: ${settings.version}
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-masters-cluster:
          id: ${@i.id}
          master: false
          hostname: node${@i.id}-${env.domain}
          version: ${settings.version}
  - upgrade-jps-manifests: ${settings.version}
  - env.control.ApplyNodeGroupData [k8sm, cp]:
      data:
        isRedeploySupport: true
  - upgrade-masters-redeploy:
      id: ${nodes.k8sm.master.id}
      master: true
      version: ${settings.version}
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-masters-redeploy:
          id: ${@i.id}
          master: false
          version: ${settings.version}
  - upgrade-masters-post:
      id: ${nodes.k8sm.master.id}
      master: true
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-masters-post:
          id: ${@i.id}
          master: false
  - forEach(nodes.cp):
      upgrade-workers:
        id: ${@i.id}
        hostname: node${@i.id}-${env.domain}
        version: ${settings.version}
  - env.control.ApplyNodeGroupData [k8sm, cp]:
      data:
        isRedeploySupport: false
  - script: |
      var message = "Kubernetes Cluster has been successfuly upgraded! **Current version:** ${settings.version}.";
      if ("${settings.avail}") { message += "\n\n**Next version:** ${settings.avail}.  \nPress \"Upgrade\" button to start the upgrade process."; }
      else { message += "\n\nNo other upgrades are available."; }
      return {result:"info", message:message};

actions:
  upgrade-configuration:
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        kubectl get daemonset weave-net -n kube-system && {
         kubectl apply -f ${baseUrl}/addons/weave-pack.yaml;
         kubectl -n kube-system wait --for=condition=Ready pod -l name=weave-net --timeout=-1s; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get daemonset traefik-ingress-controller -n kube-system &>/dev/null && echo "traefik" ||:
        kubectl get deployment nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        kubectl get daemonset nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        kubectl get daemonset haproxy-ingress -n ingress-controller &>/dev/null && echo "haproxy" ||:
    - setGlobals:
        ingress-dir: ${response.out}
    - cmd[${nodes.k8sm.master.id}]: |-
        case "x${globals.ingress-dir}" in
          xtraefik)
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-rbac.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-ds.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-ui.yaml
           kubectl -n kube-system wait --for=condition=Ready pod -l name=traefik-ingress-lb --timeout=-1s
           ;;
          xnginx)
           kubectl -n ingress-nginx delete deployment nginx-ingress-controller
           kubectl apply -f ${baseUrl}/addons/nginx/mandatory.yaml
           kubectl apply -f ${baseUrl}/addons/nginx/cloud-generic.yaml
           kubectl -n ingress-nginx wait --for=condition=Ready pod -l app.kubernetes.io/name=ingress-nginx --timeout=-1s
           ;;
          xhaproxy)
           kubectl apply -f ${baseUrl}/addons/haproxy/haproxy-deployment.yaml
           wait-deployment.sh ingress-default-backend ingress-controller 1 720
           ;;
          *)
           echo "Invalid ingress controller used!"
           exit 1
           ;;
          esac
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl apply -f ${baseUrl}/addons/metrics-server/aggregated-metrics-reader.yaml
        kubectl apply -f ${baseUrl}/addons/metrics-server/auth-delegator.yaml
        kubectl apply -f ${baseUrl}/addons/metrics-server/auth-reader.yaml
        kubectl apply -f ${baseUrl}/addons/metrics-server/metrics-apiservice.yaml
        kubectl apply -f ${baseUrl}/addons/metrics-server/metrics-server-deployment.yaml
        kubectl apply -f ${baseUrl}/addons/metrics-server/metrics-server-service.yaml
        kubectl apply -f ${baseUrl}/addons/metrics-server/resource-reader.yaml
        wait-deployment.sh metrics-server kube-system 1 720
    - cmd[${nodes.k8sm.master.id}]: |-
        # can be removed for k8s-1.18+
        kubectl get deployment kubernetes-dashboard -n kube-system && {
         kubectl delete -f ${baseUrl}/addons/kubernetes-dashboard-legacy.yaml;
         kubectl delete -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/dashboard-ingress-legacy.yaml;
         general_dashboard_upgrade="true"; } ||:
        kubectl get deployment kubernetes-dashboard -n kubernetes-dashboard && general_dashboard_upgrade="true"
        [ -n "${general_dashboard_upgrade}" ] && {
         kubectl delete ns kubernetes-dashboard;
         for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/kubernetes-dashboard.yaml && break; done;
         kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/dashboard-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get deployment hello-kubernetes && {
         kubectl delete -f ${baseUrl}/addons/helloworld.yaml;
         kubectl apply -f ${baseUrl}/addons/helloworld.yaml;
         kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/helloworld-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        helm repo update
        wait-deployment.sh tiller-deploy kube-system 1 720
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get secret --namespace kubernetes-monitoring monitoring-grafana && {
         helm upgrade monitoring-prometheus stable/prometheus --set server.prefixURL=/prometheus --set server.baseURL=/prometheus ;
         wait-deployment.sh monitoring-prometheus-server kubernetes-monitoring 1 720;
         helm fetch stable/grafana --untar;
         helm upgrade monitoring-grafana --set 'grafana\.ini'.server.root_url=${env.url}grafana -f ${baseUrl}/addons/monitoring/jelastic-values.yaml grafana/. ;
         wait-deployment.sh monitoring-grafana kubernetes-monitoring 1 720;
         kubectl apply -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/prometheus-ingress.yaml;
         kubectl apply -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/alert-ingress.yaml;
         kubectl apply -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/grafana-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: echo $(kubectl get NetworkPolicy,PodSecurityPolicy,DaemonSet,Deployment,ReplicaSet --all-namespaces -o 'jsonpath={range .items[*]}{.metadata.annotations.kubectl\.kubernetes\.io/last-applied-configuration}{"\n"}{end}' | grep '"apiVersion":"extensions/v1beta1"')
    - setGlobals:
        k8s_deprecated_ext: ${response.out}
    - if ('${globals.k8s_deprecated_ext}'):
        return:
          type: warning
          message: Deprecated Kubernetes Extensions API found! Please review installed components prior to Kubernetes 1.16+ cluster upgrade.
    - cmd[${nodes.k8sm.master.id}]: echo $(kubectl get DaemonSet,Deployment,StatefulSet,ReplicaSet --all-namespaces -o 'jsonpath={range .items[*]}{.metadata.annotations.kubectl\.kubernetes\.io/last-applied-configuration}{"\n"}{end}' | grep '"apiVersion":"apps/v1beta')
    - setGlobals:
        k8s_deprecated_app: ${response.out}
    - if ('${globals.k8s_deprecated_app}'):
        return:
          type: warning
          message: Deprecated Kubernetes Apps API found! Please review installed components prior to Kubernetes 1.16+ cluster upgrade.

  upgrade-jps-manifests:
    - version: ${this}
      manifestUrl: ${baseUrl}/manifest.jps
      envName: ${env.name}
      envAppid: ${env.appid}
      script: |
        function deepCopy(obj) { return toNative(new org.json.JSONObject(obj)); }
        function applyIf(obj, config) { for (var prop in config) { if (typeof obj[prop] == "undefined" || obj[prop] == null) { obj[prop] = config[prop]; } } return obj; }
        function schedule(jps, batch, settings) { batch.methods.push({ m : { script: "Install", params: { envName: envName, nodeGroup: nodeGroup, jps: jps, settings: settings || {}, skipEmail: true, tracked: false } } }); }
        function buildAddon(jps, parent, install) {
            var globals = deepCopy(jps.globals || {}); jps.globals = applyIf(globals, deepCopy(parent.globals));
            var actions = deepCopy(jps.actions || {}); jps.actions = applyIf(actions, deepCopy(parent.actions));
            if (!install) delete jps.onInstall;
            return jps;
        }

        var resp = api.dev.scripting.Eval("appstore", session, "GetApps", { targetAppid: envAppid, search: { appstore: 1, app_id: [ "kubernetes", { like: "%-k8s-addon" } ] } });
        resp = resp.response || resp;
        if (resp.result != 0) return resp;

        var apps = resp.apps, newAddons = {}, newJps, isInstalled;

        try {
            newJps = toNative(new org.yaml.snakeyaml.Yaml().load(new com.hivext.api.core.utils.Transport().get(manifestUrl)));

            newJps.type = "update";
            delete newJps.onBeforeInit;
            delete newJps.onBeforeInstall;
            delete newJps.onInstall;

            for (var i = 0, n = newJps.addons.length; i < n; i++ )
                newAddons[newJps.addons[i].id] = deepCopy(newJps.addons[i]);
        } catch (ex) {
            return { result: com.hivext.api.Response.OBJECT_FORMAT_ERROR, error: "Kubernetes manifest is corrupted!", data: ex };
        }

        var batch = { alias : { m : "Development.Scripting.Eval" }, global : { appid: "appstore", session: session }, methods: [] };

        for (var i = 0, n = apps.length; i < n; i++) {
            var app = apps[i], nodeGroup = "", jps;
            app.settings.data = app.settings.data || {};
            app.settings.data.version = version;

            if (app.app_id == "kubernetes") {
                jps = newJps;
                isInstalled = true;
            } else {
                jps = newAddons[app.app_id];
                nodeGroup = "k8sm";

                if (jps) {
                    jps = buildAddon(jps, newJps);
                    delete newAddons[app.app_id];
                } else {
                    resp = api.marketplace.jps.Uninstall(appid, session, app.uniqueName);
                    if (resp.result != 0) return resp;
                }
            }

            if (jps) schedule(jps, batch, app.settings.data);
        }

        for (var appId in newAddons) schedule(buildAddon(newAddons[appId], newJps, true), batch);

        if (!isInstalled)
            return { result: com.hivext.api.Response.OBJECT_FORMAT_ERROR, error: "Kubernetes manifest is not found!", data: ex};

        if (batch.methods.length > 0) {
            var batchResp = api.utils.batch.Call(appid, toJSON(batch), false);
            if (batchResp.result != 0) return batchResp;
            batchResp = batchResp.response;

            for (var i = 0, n = batchResp.length; i < n; i++) {
                resp = batchResp[i].response || batchResp[i];
                if (resp.result != 0) return { result: resp.result, error: resp.error, responses: batchResp };
            }
        }

        return { result: 0 };

  upgrade-masters-cluster:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        [ -f "/usr/local/sbin/k8sm-config" ] || {
         # can be removed for k8s-1.18+
         wget -nv http://dot.jelastic.com/download/denis/k8sm-config -O /usr/local/sbin/k8sm-config;
         chmod +x /usr/local/sbin/k8sm-config; }
        /usr/local/sbin/k8sm-config -f
        kubeadm_package=kubeadm-$(echo '${this.version}' | cut -d 'v' -f2)-1.el7.x86_64
        yum update -y "http://spacewalk.jelastic.com/pub/${kubeadm_package}.rpm"
        rpm -qi "${kubeadm_package}" || exit 1
        /usr/bin/kubectl drain ${this.hostname} --ignore-daemonsets --delete-local-data || exit 1
    - if (${this.master}):
        - cmd[${this.id}]: |-
            /usr/bin/kubeadm upgrade plan --ignore-preflight-errors=all || exit 2
            /usr/bin/kubeadm upgrade apply ${this.version} || exit 2
    - if (!${this.master}):
        - cmd[${this.id}]: |-
            /usr/bin/kubeadm upgrade node --kubelet-version ${this.version}  || exit 2
    - cmd[${this.id}]: |-
        /usr/bin/kubectl uncordon ${this.hostname} || exit 3

  upgrade-masters-redeploy:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        sed -i 's/--cgroup-driver=cgroupfs//' /var/lib/kubelet/kubeadm-flags.env
        wget -nv ${baseUrl}/configs/redeploy.conf -O /etc/jelastic/redeploy.conf
        sleep 10
    - env.control.RedeployContainers:
        nodeId: ${this.id}
        tag: ${this.version}
    - cmd[${this.id}]: |-
        init-instance.sh --type=master --base-url=$(echo '${baseUrl}' | base64 -w 0)
        [ -f "/root/wait-deployment.sh" ] && {
         rm -f /root/wait-deployment.sh;
         ln -s /usr/local/sbin/wait-deployment.sh /root/wait-deployment.sh; } ||:
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
    - cmd[${this.id}]: |-
        systemctl restart systemd-journald.service
        systemctl restart kubelet.service
        systemctl enable kubelet.service

  upgrade-masters-post:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
    - script: 'return { result : 0 };'
    - if (${this.master}):
        - cmd[${this.id}]: |-
            /usr/local/sbin/helm-install.sh --type=master | tee -a /var/log/kubernetes/k8s-helm-master.log
            wait-deployment.sh tiller-deploy kube-system 1 720
            /usr/local/sbin/install-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --metallb=true
        - cmd[${this.id}]: |-
            [ -d "/var/lib/kubelet/worker-data" ] || mkdir -p /var/lib/kubelet/worker-data
            tar zcfv - /var/lib/kubelet/worker-data 2>/dev/null | base64 -w 0
        - setGlobals:
            worker_integration: ${response.out}
    - if (!${this.master}):
        - cmd[${this.id}]: /usr/local/sbin/helm-install.sh --type=slave | tee -a /var/log/kubernetes/k8s-helm-slave.log
    - cmd[${this.id}]: |-
        /usr/local/sbin/master-postconfig.sh

  upgrade-workers:
    - cmd[${nodes.k8sm.master.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        sleep 10
        /usr/bin/kubectl drain ${this.hostname} --ignore-daemonsets --delete-local-data || exit 3
    - cmd[${this.id}]: |-
        kubeadm_package=kubeadm-$(echo '${this.version}' | cut -d 'v' -f2)-1.el7.x86_64
        yum update -y "http://spacewalk.jelastic.com/pub/${kubeadm_package}.rpm"
        rpm -qi "${kubeadm_package}" || exit 1
        /usr/bin/kubeadm upgrade node --kubelet-version ${this.version}  || exit 4
        sed -i 's/--cgroup-driver=cgroupfs//' /var/lib/kubelet/kubeadm-flags.env
        wget -nv ${baseUrl}/configs/redeploy.conf -O /etc/jelastic/redeploy.conf
    - env.control.RedeployContainers:
        nodeId: ${this.id}
        tag: ${this.version}
    - cmd[${this.id}]: |-
        init-instance.sh --type=worker --base-url=$(echo '${baseUrl}' | base64 -w 0)
        mkdir /var/lib/worker
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
        echo '${globals.worker_integration}' | base64 -d | tar zxv --strip-components=4 -C /var/lib/worker
        /usr/local/sbin/worker-integration.sh | tee -a /var/log/kubernetes/k8s-worker-integration.log
        systemctl restart systemd-journald.service
        systemctl restart kubelet.service
        systemctl enable kubelet.service
    - cmd[${nodes.k8sm.master.id}]: |-
        /usr/bin/kubectl uncordon ${this.hostname} || exit 5
        while true; do echo "$(kubectl get pods --field-selector=status.phase=Pending -n kube-system)" | grep -q Pending || break; sleep 3; done

success:
  email: false
  text: "K8s upgrade complete"
