version: 1.5
type: update
id: kubernetes-upgrade-to-1.20.4
name: Kubernetes Upgrade to 1.20.4

categories:
  - apps/dev-and-admin-tools

homepage: https://github.com/jelastic-jps/kubernetes
baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/v1.20.4
logo: /images/k8s-logo.png

description:
  text: "K8s upgrade"
  short: Kubernetes Upgrade

onInstall:
  - check-cluster-status
  - upgrade-configuration
  - init-globals-workers
  - upgrade-masters-cluster:
      id: ${nodes.k8sm.master.id}
      master: true
      hostname: node${nodes.k8sm.master.id}-${env.domain}
      version: ${settings.version}
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-masters-cluster:
          id: ${@i.id}
          master: false
          hostname: node${@i.id}-${env.domain}
          version: ${settings.version}
  - upgrade-jps-manifests: ${settings.version}
  - env.control.ApplyNodeGroupData [k8sm]:
      data:
        isRedeploySupport: true
  - upgrade-masters-redeploy:
      id: ${nodes.k8sm.master.id}
      master: true
      version: ${settings.version}
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-masters-redeploy:
          id: ${@i.id}
          master: false
          version: ${settings.version}
  - upgrade-masters-post:
      id: ${nodes.k8sm.master.id}
      master: true
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-masters-post:
          id: ${@i.id}
          master: false
  - env.control.ApplyNodeGroupData [k8sm]:
      data:
        isRedeploySupport: false
  - env.control.ApplyNodeGroupData [${globals.workers}]:
      data:
        isRedeploySupport: true
  - forEach(group:globals.workerSet):
      - forEach(node:nodes.${@group}):
          upgrade-workers:
            id: ${@node.id}
            hostname: node${@node.id}-${env.domain}
            version: ${settings.version}
  - env.control.ApplyNodeGroupData [${globals.workers}]:
      data:
        isRedeploySupport: false
  - script: |
      var message = "Kubernetes Cluster has been successfuly upgraded! **Current version:** ${settings.version}.";
      if ("${settings.avail}") { message += "\n\n**Next version:** ${settings.avail}.  \nPress \"Upgrade\" button to start the upgrade process."; }
      else { message += "\n\nNo other upgrades are available."; }
      return {result:"info", message:message};

actions:
  check-cluster-status:
    - cmd[${nodes.k8sm.master.id}]: |-
        for cluster_comp in "Kubernetes master" "KubeDNS"; do
         echo "$(TERM=dumb kubectl cluster-info 2>/dev/null)" | grep -q "${cluster_comp} is running" || echo "false";
        done
    - setGlobals:
        k8s_cluster_status: ${response.out}
    - cmd[${nodes.k8sm.master.id}]: echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' && echo "false" || echo "true"
    - setGlobals:
        k8s_node_status: ${response.out}
    - if ('${globals.k8s_cluster_status}' || '${globals.k8s_node_status}' == 'false'):
        return:
          type: warning
          message: Kubernetes Cluster current state is not suitable for upgrade. Please check its operability, API and nodes status.

  init-globals-workers:
    - script: |
        var resp = jelastic.env.control.GetEnvInfo('${env.appid}', session),
              nodeGroups = [];

        if (resp.result != 0) return resp;

        for (var i = 0, k = resp.nodes; i < k.length; i++) {
           nodeGroup = String(k[i].nodeGroup);
           if (nodeGroups.indexOf(nodeGroup) == -1) {
             if (k[i].nodeType == "kubernetes" && nodeGroup != "k8sm") {
               nodeGroups.push(nodeGroup);
             }
           }
        }

        return { result: 0, onAfterReturn: { setGlobals: {
          workerSet: nodeGroups, workers: nodeGroups.join()
        }}}

  upgrade-configuration:
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        kubectl config set-context --current --namespace=default
        kubectl get daemonset weave-net -n kube-system && {
         kubectl apply -f ${baseUrl}/addons/weave-pack.yaml;
         kubectl -n kube-system wait --for=condition=Ready pod -l name=weave-net --timeout=-1s; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get daemonset traefik-ingress-controller -n kube-system &>/dev/null && echo "traefik" ||:
        kubectl get deployment nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        kubectl get daemonset nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        kubectl get daemonset haproxy-ingress -n ingress-controller &>/dev/null && echo "haproxy" ||:
        kubectl get daemonset haproxy-ingress -n haproxy-controller &>/dev/null && echo "haproxy" ||:
    - setGlobals:
        ingress-dir: ${response.out}
    - cmd[${nodes.k8sm.master.id}]: |-
        case "x${globals.ingress-dir}" in
          xtraefik)
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-rbac.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-ds.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-ui.yaml
           kubectl -n kube-system wait --for=condition=Ready pod -l name=traefik-ingress-lb --timeout=-1s
           ;;
          xnginx)
           kubectl -n ingress-nginx delete deployment nginx-ingress-controller
           kubectl delete ns ingress-nginx
           for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/nginx/nginx-deployment.yaml && break; done;
           while [[ $(kubectl -n ingress-nginx get pods -l app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/component=controller -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done
           ;;
          xhaproxy)
           kubectl delete ns ingress-controller
           for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/haproxy/haproxy-deployment.yaml && break; done;
           while [[ $(kubectl -n haproxy-controller get pods -l run=haproxy-ingress -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done
           wait-deployment.sh ingress-default-backend haproxy-controller 1 720
           ;;
          *)
           echo "Invalid ingress controller used!"
           exit 1
           ;;
          esac
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl apply -f ${baseUrl}/addons/metrics-server.yaml
        wait-deployment.sh metrics-server kube-system 1 720
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get deployment kubernetes-dashboard -n kubernetes-dashboard && {
         kubectl delete ns kubernetes-dashboard;
         for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/kubernetes-dashboard.yaml && break; done;
         kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/dashboard-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get deployment kubernetes-k8dash -n kube-system && {
         kubectl -n kube-system delete deployment kubernetes-k8dash;
         kubectl -n kube-system delete service kubernetes-k8dash;
         kubectl -n kube-system delete ingress kubernetes-k8dash;
         for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/kubernetes-k8dash.yaml && break; done; } ||:
        kubectl get deployment kubernetes-k8dash -n kubernetes-k8dash && {
         kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/k8dash-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get ingress kubernetes-api -n default && {
         kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/api-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get deployment hello-kubernetes && {
         kubectl delete -f ${baseUrl}/addons/helloworld.yaml;
         kubectl apply -f ${baseUrl}/addons/helloworld.yaml;
         kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/helloworld-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        helm repo add deliveryhero https://charts.deliveryhero.io/
        helm repo update
        helm list | grep -q "node-problem-detector" && npd_action="upgrade" || npd_action="install"
        helm ${npd_action} node-problem-detector deliveryhero/node-problem-detector
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get secret --namespace kubernetes-monitoring monitoring-grafana && {
         grafana_base64=$(kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}');
         helm repo add prometheus-community https://prometheus-community.github.io/helm-charts;
         helm repo add grafana https://grafana.github.io/helm-charts;
         helm repo update;
         helm upgrade --namespace kubernetes-monitoring monitoring-prometheus prometheus-community/prometheus --set server.prefixURL=/prometheus --set server.baseURL=/prometheus ;
         wait-deployment.sh monitoring-prometheus-server kubernetes-monitoring 1 720;
         rm -rf /root/grafana && helm fetch grafana/grafana --untar;
         helm upgrade --namespace kubernetes-monitoring monitoring-grafana --set 'grafana\.ini'.server.root_url=${env.url}grafana -f ${baseUrl}/addons/monitoring/jelastic-values.yaml grafana/. ;
         wait-deployment.sh monitoring-grafana kubernetes-monitoring 1 720;
         kubectl patch secret --namespace kubernetes-monitoring monitoring-grafana -p="{\"data\":{\"admin-password\":\"${grafana_base64}\"}}";
         kubectl apply -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/prometheus-ingress.yaml;
         kubectl apply -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/alert-ingress.yaml;
         kubectl apply -f ${baseUrl}/addons/monitoring/${globals.ingress-dir}/grafana-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get secret --namespace observability observability-jaeger-plain && {
         kubectl apply -f ${baseUrl}/addons/ingress/${globals.ingress-dir}/jaeger-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: echo $(kubectl get NetworkPolicy,PodSecurityPolicy,DaemonSet,Deployment,ReplicaSet --all-namespaces -o 'jsonpath={range .items[*]}{.metadata.annotations.kubectl\.kubernetes\.io/last-applied-configuration}{"\n"}{end}' | grep '"apiVersion":"extensions/v1beta1"')
    - setGlobals:
        k8s_deprecated_ext: ${response.out}
    - if ('${globals.k8s_deprecated_ext}'):
        return:
          type: warning
          message: Deprecated Kubernetes Extensions API found! Please review installed components prior to Kubernetes 1.16+ cluster upgrade.
    - cmd[${nodes.k8sm.master.id}]: echo $(kubectl get DaemonSet,Deployment,StatefulSet,ReplicaSet --all-namespaces -o 'jsonpath={range .items[*]}{.metadata.annotations.kubectl\.kubernetes\.io/last-applied-configuration}{"\n"}{end}' | grep '"apiVersion":"apps/v1beta')
    - setGlobals:
        k8s_deprecated_app: ${response.out}
    - if ('${globals.k8s_deprecated_app}'):
        return:
          type: warning
          message: Deprecated Kubernetes Apps API found! Please review installed components prior to Kubernetes 1.16+ cluster upgrade.

  upgrade-jps-manifests:
    - version: ${this}
      manifestUrl: ${baseUrl}/manifest.jps
      envName: ${env.name}
      envAppid: ${env.appid}
      script: |
        function deepCopy(obj) { return toNative(new org.json.JSONObject(String(obj))); }
        function applyIf(obj, config) { for (var prop in config) { if (typeof obj[prop] == "undefined" || obj[prop] == null) { obj[prop] = config[prop]; } } return obj; }
        function schedule(jps, batch, settings) { batch.methods.push({ m : { script: "Install", params: { envName: envName, nodeGroup: nodeGroup, jps: jps, settings: settings || {}, skipEmail: true, tracked: false } } }); }
        function buildAddon(jps, parent, install) {
            var globals = deepCopy(jps.globals || {}); jps.globals = applyIf(globals, deepCopy(parent.globals));
            var actions = deepCopy(jps.actions || {}); jps.actions = applyIf(actions, deepCopy(parent.actions));
            if (!install) delete jps.onInstall;
            return jps;
        }

        var resp = api.dev.scripting.Eval("appstore", session, "GetApps", { targetAppid: envAppid, search: { appstore: 1, app_id: [ "kubernetes", "kubernetes-release", { like: "%-k8s-addon" } ] } });
        resp = resp.response || resp;
        if (resp.result != 0) return resp;

        var apps = resp.apps, newAddons = {}, newJps, isInstalled;

        try {
            newJps = toNative(new org.yaml.snakeyaml.Yaml().load(new com.hivext.api.core.utils.Transport().get(manifestUrl)));

            newJps.type = "update";
            delete newJps.onBeforeInit;
            delete newJps.onBeforeInstall;
            delete newJps.onInstall;

            for (var i = 0, n = newJps.addons.length; i < n; i++ )
                newAddons[newJps.addons[i].id] = deepCopy(newJps.addons[i]);
        } catch (ex) {
            return { result: com.hivext.api.Response.OBJECT_FORMAT_ERROR, error: "Kubernetes manifest is corrupted!", data: ex };
        }

        var batch = { alias : { m : "Development.Scripting.Eval" }, global : { appid: "appstore", session: session }, methods: [] };

        for (var i = 0, n = apps.length; i < n; i++) {
            var app = apps[i], nodeGroup = "", jps;

            if (!app.isInstalled) continue;
            app.settings.data = app.settings.data || {};
            app.settings.data.version = version;

            if (app.app_id == "kubernetes" || app.app_id == "kubernetes-release") {
                jps = newJps;
                isInstalled = true;
            } else {
                jps = newAddons[app.app_id];
                nodeGroup = "k8sm";

                if (jps) {
                    jps = buildAddon(jps, newJps);
                    delete newAddons[app.app_id];
                } else {
                    resp = api.marketplace.jps.Uninstall(appid, session, app.uniqueName);
                    if (resp.result != 0) return resp;
                }
            }

            if (jps) schedule(jps, batch, app.settings.data);
        }

        for (var appId in newAddons) schedule(buildAddon(newAddons[appId], newJps, true), batch);

        if (!isInstalled)
            return { result: com.hivext.api.Response.OBJECT_FORMAT_ERROR, error: "Kubernetes manifest is not found!", data: ex};

        if (batch.methods.length > 0) {
            var batchResp = api.utils.batch.Call(appid, toJSON(batch), false);
            if (batchResp.result != 0) return batchResp;
            batchResp = batchResp.response;

            for (var i = 0, n = batchResp.length; i < n; i++) {
                resp = batchResp[i].response || batchResp[i];
                if (resp.result != 0) return { result: resp.result, error: resp.error, responses: batchResp };
            }
        }

        return { result: 0 };

  upgrade-masters-cluster:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        /usr/local/sbin/k8sm-config -f
        kubeadm_package=kubeadm-$(echo '${this.version}' | cut -d 'v' -f2)-1.el7.x86_64
        yum update -y "https://app-artifacts.s3.eu-central-1.amazonaws.com/kubernetes/${kubeadm_package}.rpm"
        rpm -qi "${kubeadm_package}" || exit 1
        /usr/bin/kubectl drain ${this.hostname} --ignore-daemonsets --delete-local-data || exit 1
    - if (${this.master}):
        - cmd[${this.id}]: |-
            /usr/bin/kubeadm upgrade plan --ignore-preflight-errors=all || exit 2
            /usr/bin/kubeadm upgrade apply ${this.version} || exit 2
    - if (!${this.master}):
        - cmd[${this.id}]: |-
            /usr/bin/kubeadm upgrade node || exit 2
    - cmd[${this.id}]: |-
        grep -q '^failSwapOn:' /var/lib/kubelet/config.yaml || sed -i '/^kind: KubeletConfiguration/a failSwapOn: false' /var/lib/kubelet/config.yaml
        /usr/bin/kubectl uncordon ${this.hostname} || exit 3

  upgrade-masters-redeploy:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        wget -nv ${baseUrl}/configs/redeploy.conf -O /tmp/redeploy.conf.stock
        sed -i -e '$a\' /tmp/redeploy.conf.stock
        cat /tmp/redeploy.conf.stock /etc/jelastic/redeploy.conf > /tmp/redeploy.conf.merged
        sed '/^$/d' /tmp/redeploy.conf.merged | sort | uniq > /etc/jelastic/redeploy.conf
        # drop in k8s-1.21+
        sed -i '/^\/var\/lib\/docker$/d' /etc/jelastic/redeploy.conf
        /usr/bin/grep -q "\-\-container\-runtime=remote" /var/lib/kubelet/kubeadm-flags.env || \
         sed -i "/^KUBELET_KUBEADM_ARGS/ s/\"$/ --container-runtime=remote --container-runtime-endpoint=unix:\/\/\/run\/containerd\/containerd.sock\"/" /var/lib/kubelet/kubeadm-flags.env
        sed -i 's%\(.*\)--pod-infra-container-image=[^[:space:]\"]\+%\1%g' /var/lib/kubelet/kubeadm-flags.env
        sleep 10
    - env.control.RedeployContainers:
        nodeId: ${this.id}
        tag: ${this.version}
    - cmd[${this.id}]: |-
        init-instance.sh --type=master --base-url=$(echo '${baseUrl}' | base64 -w 0)
        [ -f "/root/wait-deployment.sh" ] && {
         rm -f /root/wait-deployment.sh; } ||:
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
    - cmd[${this.id}]: |-
        systemctl restart systemd-journald.service
        systemctl restart kubelet.service
        systemctl enable kubelet.service

  upgrade-masters-post:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
    - script: 'return { result : 0 };'
    - if (${this.master}):
        - cmd[${this.id}]: |-
            /usr/local/sbin/helm-install.sh | tee -a /var/log/kubernetes/k8s-helm-master.log
            /usr/local/sbin/install-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --metallb=true
        - cmd[${this.id}]: |-
            [ -d "/var/lib/kubelet/worker-data" ] || mkdir -p /var/lib/kubelet/worker-data
            tar zcfv - /var/lib/kubelet/worker-data 2>/dev/null | base64 -w 0
        - setGlobals:
            worker_integration: ${response.out}
    - if (!${this.master}):
        - cmd[${this.id}]: /usr/local/sbin/helm-install.sh | tee -a /var/log/kubernetes/k8s-helm-slave.log
    - cmd[${this.id}]: |-
        if [ -f "$HOME/.kube/config" ]; then
          client_certificate_data=$(grep 'client-certificate-data' /etc/kubernetes/admin.conf | awk '{print $2}')
          client_key_data=$(grep 'client-key-data' /etc/kubernetes/admin.conf | awk '{print $2}')
          [ -n "$client_certificate_data" ] && sed -i -r "s/^(\s*client-certificate-data:\s*)(.*$)/\1${client_certificate_data}/" $HOME/.kube/config
          [ -n "$client_key_data" ] && sed -i -r "s/^(\s*client-key-data:\s*)(.*$)/\1${client_key_data}/" $HOME/.kube/config
        else
          mkdir -p $HOME/.kube
          /usr/bin/cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
        fi
        chown root:root $HOME/.kube/config
        /usr/local/sbin/master-postconfig.sh

  upgrade-workers:
    - cmd[${nodes.k8sm.master.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        sleep 10
        /usr/bin/kubectl drain ${this.hostname} --ignore-daemonsets --delete-emptydir-data || exit 3
    - cmd[${this.id}]: |-
        kubeadm_package=kubeadm-$(echo '${this.version}' | cut -d 'v' -f2)-1.el7.x86_64
        yum update -y "https://app-artifacts.s3.eu-central-1.amazonaws.com/kubernetes/${kubeadm_package}.rpm"
        rpm -qi "${kubeadm_package}" || exit 1
        /usr/bin/kubeadm upgrade node || exit 4
        wget -nv ${baseUrl}/configs/redeploy.conf -O /tmp/redeploy.conf.stock
        sed -i -e '$a\' /tmp/redeploy.conf.stock
        cat /tmp/redeploy.conf.stock /etc/jelastic/redeploy.conf > /tmp/redeploy.conf.merged
        sed '/^$/d' /tmp/redeploy.conf.merged | sort | uniq > /etc/jelastic/redeploy.conf
        # drop in k8s-1.21+
        sed -i '/^\/var\/lib\/docker$/d' /etc/jelastic/redeploy.conf
        /usr/bin/grep -q "\-\-container\-runtime=remote" /var/lib/kubelet/kubeadm-flags.env || \
         sed -i "/^KUBELET_KUBEADM_ARGS/ s/\"$/ --container-runtime=remote --container-runtime-endpoint=unix:\/\/\/run\/containerd\/containerd.sock\"/" /var/lib/kubelet/kubeadm-flags.env
        sed -i 's%\(.*\)--pod-infra-container-image=[^[:space:]\"]\+%\1%g' /var/lib/kubelet/kubeadm-flags.env
    - env.control.RedeployContainers:
        nodeId: ${this.id}
        tag: ${this.version}
    - cmd[${this.id}]: |-
        init-instance.sh --type=worker --base-url=$(echo '${baseUrl}' | base64 -w 0)
        mkdir /var/lib/worker
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
        echo '${globals.worker_integration}' | base64 -d | tar zxv --strip-components=4 -C /var/lib/worker
        /usr/local/sbin/worker-integration.sh | tee -a /var/log/kubernetes/k8s-worker-integration.log
        systemctl restart systemd-journald.service
        systemctl restart kubelet.service
        systemctl enable kubelet.service
    - cmd[${nodes.k8sm.master.id}]: |-
        /usr/bin/kubectl uncordon ${this.hostname} || exit 5
        while true; do echo "$(kubectl get pods --field-selector=status.phase=Pending -n kube-system)" | grep -q Pending || break; sleep 3; done

success:
  email: false
  text: "K8s upgrade complete"
