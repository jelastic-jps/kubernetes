version: 1.5
type: update
id: kubernetes-upgrade-to-1.29.8
name: Kubernetes Upgrade to 1.29.8

categories:
  - apps/dev-and-admin-tools

homepage: https://github.com/jelastic-jps/kubernetes
baseUrl: https://raw.githubusercontent.com/dimkadt/kubernetes/v1.29.8
logo: /images/k8s-logo.png

description:
  text: "K8s upgrade"
  short: Kubernetes Upgrade

onInstall:
  - check-cluster-status
  - upgrade-configuration
  - init-globals-workers
  - upgrade-cplanes-cluster:
      id: ${nodes.k8sm.master.id}
      main: true
      hostname: node${nodes.k8sm.master.id}-${env.domain}
      version: ${settings.version}
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-cplanes-cluster:
          id: ${@i.id}
          main: false
          hostname: node${@i.id}-${env.domain}
          version: ${settings.version}
  - upgrade-jps-manifests: ${settings.version}
  - env.control.ApplyNodeGroupData [k8sm]:
      data:
        isRedeploySupport: true
  - upgrade-cplanes-redeploy:
      id: ${nodes.k8sm.master.id}
      main: true
      version: ${settings.version}
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-cplanes-redeploy:
          id: ${@i.id}
          main: false
          version: ${settings.version}
  - upgrade-cplanes-post:
      id: ${nodes.k8sm.master.id}
      main: true
  - forEach(nodes.k8sm):
      if (!${@i.ismaster}):
        upgrade-cplanes-post:
          id: ${@i.id}
          main: false
  - env.control.ApplyNodeGroupData [k8sm]:
      data:
        isRedeploySupport: false
  - env.control.ApplyNodeGroupData [${globals.workers}]:
      data:
        isRedeploySupport: true
  - forEach(group:globals.workerSet):
      - forEach(node:nodes.${@group}):
          upgrade-workers:
            id: ${@node.id}
            group: ${@group}
            hostname: node${@node.id}-${env.domain}
            version: ${settings.version}
  - env.control.ApplyNodeGroupData [${globals.workers}]:
      data:
        validation:
          scalingMode: "stateless"
        isRedeploySupport: false
  - env.file.AddFavorite [k8sm, ${globals.workers}]:
        path: /etc/jelastic/redeploy.conf
        isDir: false
  - script: |
      var message = "Kubernetes Cluster has been successfuly upgraded! **Current version:** ${settings.version}.";
      if ("${settings.avail}") { message += "\n\n**Next version:** ${settings.avail}.  \nPress \"Upgrade\" button to start the upgrade process."; }
      else { message += "\n\nNo other upgrades are available."; }
      return {result:"info", message:message};

actions:
  check-cluster-status:
    - cmd[${nodes.k8sm.master.id}]: |-
        for cluster_comp in "^Kubernetes control plane" "^(KubeDNS|CoreDNS)"; do
         echo "$(TERM=dumb kubectl cluster-info 2>/dev/null)" | grep -Eq "${cluster_comp} is running" || echo "false";
        done
    - set:
        k8s_cluster_status: ${response.out}
    - cmd[${nodes.k8sm.master.id}]: echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' && echo "false" || echo "true"
    - set:
        k8s_node_status: ${response.out}
    - if ('${this.k8s_cluster_status}' || '${this.k8s_node_status}' == 'false'):
        return:
          type: warning
          message: Kubernetes Cluster current state is not suitable for upgrade. Please check its operability, API and nodes status.

  init-globals-workers:
    - script: |
        var resp = jelastic.env.control.GetEnvInfo('${env.appid}', session),
              nodeGroups = [];

        if (resp.result != 0) return resp;

        for (var i = 0, k = resp.nodes; i < k.length; i++) {
           nodeGroup = String(k[i].nodeGroup);
           if (nodeGroups.indexOf(nodeGroup) == -1) {
             if (k[i].nodeType == "kubernetes" && nodeGroup != "k8sm") {
               nodeGroups.push(nodeGroup);
             }
           }
        }

        return { result: 0, onAfterReturn: { setGlobals: {
          workerSet: nodeGroups, workers: nodeGroups.join()
        }}}

  upgrade-configuration:
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        kubectl config set-context --current --namespace=default
        kubectl get daemonset weave-net -n kube-system && {
         kubectl apply -f ${baseUrl}/addons/weave-pack.yaml;
         kubectl -n kube-system wait --for=condition=Ready pod -l name=weave-net --timeout=-1s; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get daemonset traefik-ingress-controller -n ingress-traefik &>/dev/null && echo "traefik" ||:
        kubectl get daemonset nginx-ingress-controller -n ingress-nginx &>/dev/null && echo "nginx" ||:
        kubectl get daemonset haproxy-ingress -n haproxy-controller &>/dev/null && echo "haproxy" ||:
    - set:
        ingress-dir: ${response.out}
    - cmd[${nodes.k8sm.master.id}]: |-
        case "x${this.ingress-dir}" in
          xtraefik)
           kubectl delete ns ingress-traefik
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-ns.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-crd.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-rbac.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-ds.yaml
           kubectl apply -f ${baseUrl}/addons/traefik/traefik-class.yaml
           while [[ $(kubectl -n ingress-traefik get pods -l app.kubernetes.io/name=ingress-traefik,app.kubernetes.io/component=controller -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done
           ;;
          xnginx)
           kubectl delete ns ingress-nginx
           for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/nginx/nginx-deployment.yaml && break; done;
           while [[ $(kubectl -n ingress-nginx get pods -l app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/component=controller -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done
           while true; do kubectl get -A ValidatingWebhookConfiguration && break; sleep 5; done
           while true; do kubectl -n ingress-nginx get svc ingress-nginx-controller-admission && break; sleep 5; done
           ;;
          xhaproxy)
           kubectl delete ns haproxy-controller
           for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/haproxy/haproxy-deployment.yaml && break; done;
           while [[ $(kubectl -n haproxy-controller get pods -l run=haproxy-ingress -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True"* ]]; do sleep 5; done
           wait-deployment.sh ingress-default-backend haproxy-controller 1 720
           ;;
          *)
           echo "Unknown ingress controller used, skipped"
           ;;
          esac
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl apply -f ${baseUrl}/addons/metrics-server.yaml
        wait-deployment.sh metrics-server kube-system 1 720
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get deployment kubernetes-dashboard -n kubernetes-dashboard && {
         kubectl delete ns kubernetes-dashboard;
         for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/kubernetes-dashboard.yaml && break; done;
         [ -n "${this.ingress-dir}" ] && kubectl apply -f ${baseUrl}/addons/ingress/${this.ingress-dir}/dashboard-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get deployment kubernetes-skooner -n kubernetes-skooner && {
         kubectl delete ns kubernetes-skooner;
         for i in {1..5}; do sleep 5; echo "Attempt ${i}/5: "; kubectl apply -f ${baseUrl}/addons/kubernetes-skooner.yaml && break; done;
         [ -n "${this.ingress-dir}" ] && kubectl apply -f ${baseUrl}/addons/ingress/${this.ingress-dir}/skooner-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get ingress kubernetes-api -n default && {
         [ -n "${this.ingress-dir}" ] && kubectl apply -f ${baseUrl}/addons/ingress/${this.ingress-dir}/api-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get deployment hello-kubernetes && {
         kubectl delete -f ${baseUrl}/addons/helloworld.yaml;
         kubectl apply -f ${baseUrl}/addons/helloworld.yaml;
         [ -n "${this.ingress-dir}" ] && kubectl apply -f ${baseUrl}/addons/ingress/${this.ingress-dir}/helloworld-ingress.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        helm list | grep -q "nfs-client-provisioner" && {
         helm get values nfs-client-provisioner -o yaml > /tmp/nfs-provisioner-options.yaml;
         helm delete nfs-client-provisioner;
         helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/;
         helm repo update;
         helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner -f /tmp/nfs-provisioner-options.yaml; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        helm list | grep -q "nfs-subdir-external-provisioner" && {
         helm repo update;
         helm upgrade nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        helm repo add deliveryhero https://charts.deliveryhero.io/
        helm repo update
        helm list | grep -q "node-problem-detector" && npd_action="upgrade" || npd_action="install"
        helm ${npd_action} node-problem-detector deliveryhero/node-problem-detector
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get secret --namespace kubernetes-monitoring monitoring-grafana && {
         grafana_base64=$(kubectl get secret --namespace kubernetes-monitoring monitoring-grafana -o jsonpath='{.data.admin-password}');
         helm repo add prometheus-community https://prometheus-community.github.io/helm-charts;
         helm repo add grafana https://grafana.github.io/helm-charts;
         helm repo update;
         helm upgrade --namespace kubernetes-monitoring monitoring-prometheus prometheus-community/prometheus --set server.prefixURL=/prometheus --set server.baseURL=/prometheus ;
         wait-deployment.sh monitoring-prometheus-server kubernetes-monitoring 1 720;
         rm -rf /root/grafana && helm fetch grafana/grafana --untar;
         helm upgrade --namespace kubernetes-monitoring monitoring-grafana --set 'grafana\.ini'.server.root_url=${env.url}grafana -f ${baseUrl}/addons/monitoring/jelastic-values.yaml grafana/. ;
         wait-deployment.sh monitoring-grafana kubernetes-monitoring 1 720;
         kubectl patch secret --namespace kubernetes-monitoring monitoring-grafana -p="{\"data\":{\"admin-password\":\"${grafana_base64}\"}}";
         if [ -n "${this.ingress-dir}" ]; then
          kubectl apply -f ${baseUrl}/addons/monitoring/${this.ingress-dir}/prometheus-ingress.yaml;
          kubectl apply -f ${baseUrl}/addons/monitoring/${this.ingress-dir}/alert-ingress.yaml;
          kubectl apply -f ${baseUrl}/addons/monitoring/${this.ingress-dir}/grafana-ingress.yaml; fi; } ||:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl get secret --namespace observability observability-jaeger-plain && {
         [ -n "${this.ingress-dir}" ] && kubectl apply -f ${baseUrl}/addons/ingress/${this.ingress-dir}/jaeger-ingress.yaml; } ||:

  upgrade-jps-manifests:
    - version: ${this}
      manifestUrl: ${baseUrl}/manifest.jps
      envName: ${env.name}
      envAppid: ${env.appid}
      script: |
        function deepCopy(obj) { return toNative(new org.json.JSONObject(String(obj))); }
        function applyIf(obj, config) { for (var prop in config) { if (typeof obj[prop] == "undefined" || obj[prop] == null) { obj[prop] = config[prop]; } } return obj; }
        function schedule(jps, batch, settings, nodeGroup) { batch.methods.push({ m : { script: "Install", params: { envName: envName, nodeGroup: nodeGroup || "", jps: jps, settings: settings || {}, skipEmail: true, tracked: false } } }); }
        function buildAddon(jps, parent, install) {
            var globals = deepCopy(jps.globals || {}); jps.globals = applyIf(globals, deepCopy(parent.globals));
            var actions = deepCopy(jps.actions || {}); jps.actions = applyIf(actions, deepCopy(parent.actions));
            if (!install) delete jps.onInstall;
            return jps;
        }

        var resp = api.dev.scripting.Eval("appstore", session, "GetApps", { targetAppid: envAppid, search: { appstore: 1, app_id: [ "kubernetes", "kubernetes-release", { like: "%-k8s-addon" } ] } });
        resp = resp.response || resp;
        if (resp.result != 0) return resp;

        var apps = resp.apps, newAddons = {}, mainApp = {}, mainGroup = "k8sm", newJps;

        try {
            newJps = toNative(new org.yaml.snakeyaml.Yaml().load(new com.hivext.api.core.utils.Transport().get(manifestUrl)));

            newJps.type = "update";
            delete newJps.onBeforeInit;
            delete newJps.onBeforeInstall;
            delete newJps.onInstall;

            for (var i = 0, n = newJps.addons.length; i < n; i++ )
                newAddons[newJps.addons[i].id] = deepCopy(newJps.addons[i]);
        } catch (ex) {
            return { result: com.hivext.api.Response.OBJECT_FORMAT_ERROR, error: "Kubernetes manifest is corrupted!", data: ex };
        }

        var batch = { alias : { m : "Development.Scripting.Eval" }, global : { appid: "appstore", session: session }, methods: [] };

        for (var i = 0, n = apps.length; i < n; i++) {
            var app = apps[i], jps;

            if (!app.isInstalled) continue;
            app.settings.data = app.settings.data || {};
            app.settings.data.version = version;

            if (app.app_id == "kubernetes" || app.app_id == "kubernetes-release") {
                mainApp[app.app_id] = { jps: newJps, data: app.settings.data };
            } else {
                jps = newAddons[app.app_id];

                if (jps) {
                    jps = buildAddon(jps, newJps);
                    delete newAddons[app.app_id];

                    schedule(jps, batch, app.settings.data, mainGroup);
                } else {
                    resp = api.marketplace.jps.Uninstall({ appUniqueName: app.uniqueName, force: true });
                    if (resp.result != 0) return resp;
                }
            }
        }

        if (mainApp) {
            mainApp = mainApp["kubernetes-release"] || mainApp.kubernetes;
            schedule(mainApp.jps, batch, mainApp.data);
        } else return { result: com.hivext.api.Response.OBJECT_FORMAT_ERROR, error: "Kubernetes manifest is not found!", data: ex};

        for (var appId in newAddons) schedule(buildAddon(newAddons[appId], newJps, true), batch, null, mainGroup);

        if (batch.methods.length > 0) {
            var batchResp = api.utils.batch.Call(appid, toJSON(batch), false);
            if (batchResp.result != 0) return batchResp;
            batchResp = batchResp.response;

            for (var i = 0, n = batchResp.length; i < n; i++) {
                resp = batchResp[i].response || batchResp[i];
                if (resp.result != 0) return { result: resp.result, error: resp.error, responses: batchResp };
            }
        }

        return { result: 0 };

  upgrade-cplanes-cluster:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        /usr/local/sbin/k8sm-config -f
        kubeadm_package=kubeadm-$(echo '${this.version}' | cut -d 'v' -f2)-1.el7.x86_64
        yum update -y "https://repository.jelastic.com/pub/k8s/${kubeadm_package}.rpm"
        rpm -qi "${kubeadm_package}" || exit 1
        /usr/bin/kubectl drain ${this.hostname} --ignore-daemonsets --delete-emptydir-data || exit 1
    - if (${this.main}):
        - cmd[${this.id}]: |-
            /usr/bin/kubeadm upgrade plan --ignore-preflight-errors=all || exit 2
            /usr/bin/kubeadm upgrade apply ${this.version} || exit 2
    - if (!${this.main}):
        - cmd[${this.id}]: |-
            /usr/bin/kubeadm upgrade node || exit 2
    - cmd[${this.id}]: |-
        /usr/bin/kubectl uncordon ${this.hostname} || exit 3

  upgrade-cplanes-redeploy:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        rm -f /etc/kubernetes/custom-kubeadm.yaml
        wget -nv ${baseUrl}/configs/redeploy.conf -O /tmp/redeploy.conf.stock
        sed -i -e '$a\' /tmp/redeploy.conf.stock
        cat /tmp/redeploy.conf.stock /etc/jelastic/redeploy.conf > /tmp/redeploy.conf.merged
        sed '/^$/d' /tmp/redeploy.conf.merged | sort | uniq > /etc/jelastic/redeploy.conf
        sleep 10
    - env.control.RedeployContainers:
        nodeId: ${this.id}
        tag: ${this.version}
    - cmd[${this.id}]: |-
        init-instance.sh --type=cplane --base-url=$(echo '${baseUrl}' | base64 -w 0)
        [ -f "/root/wait-deployment.sh" ] && {
         rm -f /root/wait-deployment.sh; } ||:
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
    - cmd[${this.id}]: |-
        systemctl restart systemd-journald.service
        systemctl restart kubelet.service
        systemctl enable kubelet.service

  upgrade-cplanes-post:
    - cmd[${this.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
    - script: 'return { result : 0 };'
    - if (${this.main}):
        - cmd[${this.id}]: |-
            /usr/local/sbin/helm-install.sh | tee -a /var/log/kubernetes/k8s-helm-main.log
            /usr/local/sbin/install-components.sh --base-url=$(echo '${baseUrl}' | base64 -w 0) --metallb=true
        - cmd[${this.id}]: |-
            [ -d "/var/lib/kubelet/worker-data" ] || mkdir -p /var/lib/kubelet/worker-data
            tar zcfv - /var/lib/kubelet/worker-data 2>/dev/null | base64 -w 0
        - setGlobals:
            worker_integration: ${response.out}
    - if (!${this.main}):
        - cmd[${this.id}]: |-
            /usr/local/sbin/helm-install.sh | tee -a /var/log/kubernetes/k8s-helm-secondary.log
    - cmd[${this.id}]: |-
        if [ -f "$HOME/.kube/config" ]; then
          client_certificate_data=$(grep 'client-certificate-data' /etc/kubernetes/admin.conf | awk '{print $2}')
          client_key_data=$(grep 'client-key-data' /etc/kubernetes/admin.conf | awk '{print $2}')
          [ -n "$client_certificate_data" ] && sed -i -r "s/^(\s*client-certificate-data:\s*)(.*$)/\1${client_certificate_data}/" $HOME/.kube/config
          [ -n "$client_key_data" ] && sed -i -r "s/^(\s*client-key-data:\s*)(.*$)/\1${client_key_data}/" $HOME/.kube/config
        else
          mkdir -p $HOME/.kube
          /usr/bin/cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
        fi
        chown root:root $HOME/.kube/config
        /usr/local/sbin/cplane-postconfig.sh

  upgrade-workers:
    - cmd[${nodes.k8sm.master.id}]: |-
        while true; do echo "$(kubectl get nodes --no-headers 2>/dev/null)" | grep -qv '\sReady\s' || break; sleep 3; done
        sleep 10
        /usr/bin/kubectl drain ${this.hostname} --ignore-daemonsets --delete-emptydir-data || exit 3
    - cmd[${this.id}]: |-
        kubeadm_package=kubeadm-$(echo '${this.version}' | cut -d 'v' -f2)-1.el7.x86_64
        yum update -y "https://repository.jelastic.com/pub/k8s/${kubeadm_package}.rpm"
        rpm -qi "${kubeadm_package}" || exit 1
        /usr/bin/kubeadm upgrade node || exit 4
        rm -f /etc/kubernetes/custom-kubeadm.yaml
        wget -nv ${baseUrl}/configs/redeploy.conf -O /tmp/redeploy.conf.stock
        sed -i -e '$a\' /tmp/redeploy.conf.stock
        cat /tmp/redeploy.conf.stock /etc/jelastic/redeploy.conf > /tmp/redeploy.conf.merged
        sed '/^$/d' /tmp/redeploy.conf.merged | sort | uniq > /etc/jelastic/redeploy.conf
    - env.control.RedeployContainers:
        nodeId: ${this.id}
        tag: ${this.version}
    - cmd[${this.id}]: |-
        init-instance.sh --type=worker --base-url=$(echo '${baseUrl}' | base64 -w 0)
        mkdir /var/lib/worker
        while true; do [ -f "/tmp/jelastic-conf-mark" ] && break; echo "Waiting for cluster bootstrap configuration"; sleep 3; done
        echo '${globals.worker_integration}' | base64 -d | tar zxv --strip-components=4 -C /var/lib/worker
        /usr/local/sbin/worker-integration.sh | tee -a /var/log/kubernetes/k8s-worker-integration.log
        systemctl restart systemd-journald.service
        systemctl restart kubelet.service
        systemctl enable kubelet.service
    - cmd[${nodes.k8sm.master.id}]: |-
        /usr/bin/kubectl uncordon ${this.hostname} || exit 5
        /usr/local/sbin/worker-config -n ${this.id} -g ${this.group} -r ${env.region}
        while true; do echo "$(kubectl get pods --field-selector=status.phase=Pending -n kube-system)" | grep -q Pending || break; sleep 3; done
        if kubectl get daemonset weave-net -n kube-system; then
           FAILED_PODS=$(kubectl -n kube-system get pods -l name=weave-net --no-headers | grep Error|awk '{print $1}');
           if [[ -n "$FAILED_PODS" ]] ; then
             kubectl -n kube-system delete pods $FAILED_PODS;
           fi 
        fi

success:
  email: false
  text: "K8s upgrade complete"
